{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs"
      ],
      "metadata": {
        "id": "Ty8YP4TNd3SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s27QlAzX_Qt7",
        "outputId": "90e69941-fbc1-4c76-cfed-76f078971300"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.22.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPue4B2vPu2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59cb754e-8d97-443b-9c64-337f74b1c9a9"
      },
      "source": [
        "# import standard PyTorch modules\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
        "import torch.profiler\n",
        "# import torchvision module to handle image manipulation\n",
        "import torchvision\n",
        "from torchvision import datasets \n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "# calculate train time, writing train data to files etc.\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "import copy\n",
        "from IPython.display import clear_output\n",
        "from time import perf_counter\n",
        "from torch.autograd import Variable\n",
        "torch.set_printoptions(linewidth=120)\n",
        "torch.set_grad_enabled(True)     # On by default, leave it here for clarity"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f155b86aa10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "CesBIiQHQYRL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn"
      ],
      "metadata": {
        "id": "y5Av6YqnssFz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "xQOmyV5ruNf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDlvtrBQZO3",
        "outputId": "e5c4b9df-5ce5-4908-9313-8fa6c784d8d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check PyTorch versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "0.15.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "CojWu8cMDc-j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging into WandB\n"
      ],
      "metadata": {
        "id": "kaAPJzOC_cCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TehLE0Z5_fxY",
        "outputId": "dec36348-210d-4b5a-db90-89cb657aa9de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjain-49\u001b[0m (\u001b[33mdl-codes\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset\n"
      ],
      "metadata": {
        "id": "XzxFyYxad71T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n"
      ],
      "metadata": {
        "id": "xHh1pvf9eUFw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = torchvision.datasets.CIFAR10('./data', download=True, train=True, transform=train_transforms)\n",
        "\n",
        "# Download and load the test set\n",
        "dataset_test = torchvision.datasets.CIFAR10('./data', download=True, train=False, transform=test_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971793a9-8131-47c0-ad89-1df41c933365",
        "id": "ak2BBC1oQpb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, valset = torch.utils.data.random_split(dataset_train, [40000, 10000])\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "H_0f-XwTQpb1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set cuda to device"
      ],
      "metadata": {
        "id": "W9gpIza6eIME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.cuda.memory_summary(device=None, abbreviated=True)"
      ],
      "metadata": {
        "id": "i5xtz8_DZBq8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wHJbL98ZY5mg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSi_tn2rOSKl",
        "outputId": "14ed407c-4b3b-4c03-d604-a3d0f50b59b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Some key definitions"
      ],
      "metadata": {
        "id": "Nsyem3b0eQSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count of correct predictions"
      ],
      "metadata": {
        "id": "8ozaaWJ7eXmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
        "  # obtain confidence scores for all classes\n",
        "  scores = F.softmax(outputs.data, dim=1)\n",
        "  log_scores = scores.cpu().numpy()\n",
        "  log_images = images.cpu().numpy()\n",
        "  log_labels = labels.cpu().numpy()\n",
        "  log_preds = predicted.cpu().numpy()\n",
        "  # adding ids based on the order of the images\n",
        "  _id = 0\n",
        "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
        "    # add required info to data table:\n",
        "    # id, image pixels, model's guess, true label, scores for all classes\n",
        "    img_id = str(_id) + \"_\" + str(log_counter)\n",
        "    test_table.add_data(img_id, wandb.Image(np.transpose(i, (1, 2, 0))), p, l, *s)\n",
        "    _id += 1\n",
        "    if _id == NUM_IMAGES_PER_BATCH:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "87NTgyACpIDd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOe6m7forHCM"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topk_correct(preds,labels,k=5):\n",
        "    batch_size = preds.shape[0]\n",
        "\n",
        "    # get the top-5 predictions for each sample\n",
        "    topk_values, topk_indices = torch.topk(preds, k=5, dim=1)\n",
        "\n",
        "    # check if the target is in the top-5 predictions\n",
        "    correct_topk = topk_indices.eq(labels.view(-1, 1).expand_as(topk_indices))\n",
        "    return correct_topk\n",
        "## calculate top-5 accuracy\n",
        "#top5_accuracy = correct_topk.float().sum() / batch_size"
      ],
      "metadata": {
        "id": "t6YaB1HBJkyu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## taking value of parameters and trying them in all combinations"
      ],
      "metadata": {
        "id": "jv8d5oYGecI4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBOxsN4A7N8K"
      },
      "source": [
        "# import modules to build RunBuilder and RunManager helper classes\n",
        "from collections  import OrderedDict\n",
        "from collections import namedtuple\n",
        "from itertools import product\n",
        "\n",
        "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
        "# combinations of hyper-parameters\n",
        "class RunBuilder():\n",
        "  @staticmethod\n",
        "  def get_runs(params):\n",
        "\n",
        "    Run = namedtuple('Run', params.keys())\n",
        "\n",
        "    runs = []\n",
        "    for v in product(*params.values()):\n",
        "      runs.append(Run(*v))\n",
        "    \n",
        "    return runs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## defining Manager class "
      ],
      "metadata": {
        "id": "Zyi8muZXeiMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps in calculating both train and validation accuracy and losses also help us to maintain a tensorboard and print all the data collected for every epoch and save it in a dataset "
      ],
      "metadata": {
        "id": "DVo1Jq-ge0Oa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSddFGL7zisa"
      },
      "source": [
        "# Helper class, help track loss, accuracy, epoch time, run time, \n",
        "# hyper-parameters etc. Also record to TensorBoard and write into csv, json\n",
        "class RunManager():\n",
        "  def __init__(self):\n",
        "\n",
        "    # tracking every epoch count, loss, accuracy, time\n",
        "    self.epoch_count = 0\n",
        "    self.epoch_loss = 0\n",
        "    self.epoch_val_loss = 0\n",
        "    self.epoch_num_correct_train = 0\n",
        "    self.epoch_num_correct_test = 0\n",
        "    self.epoch_start_time = None\n",
        "\n",
        "    # tracking every run count, run data, hyper-params used, time\n",
        "    self.run_params = None\n",
        "    self.run_count = 0\n",
        "    self.run_data = []\n",
        "    self.run_start_time = None\n",
        "\n",
        "    # record model, loader and TensorBoard \n",
        "    self.network = None\n",
        "    self.trainloader = None\n",
        "    self.valloader = None\n",
        "    self.tb = None\n",
        "\n",
        "  # record the count, hyper-param, model, loader of each run\n",
        "  # record sample images and network graph to TensorBoard  \n",
        "  def begin_run(self, run, network, trainloader,valloader):\n",
        "\n",
        "    self.run_start_time = time.time()\n",
        "\n",
        "    self.run_params = run\n",
        "    self.run_count += 1\n",
        "\n",
        "    self.network = network\n",
        "    self.trainloader = trainloader\n",
        "    self.valloader = valloader\n",
        "    self.tb = SummaryWriter(comment=f'-{run}')\n",
        "\n",
        "    wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project=\"dl_project_2023\",\n",
        "        \n",
        "        # track hyperparameters and run metadata\n",
        "        config={\n",
        "        \"learning_rate\": run.lr,\n",
        "        \"architecture\": run.arch,\n",
        "        \"dataset\": run.dataset,\n",
        "        \"epochs\": 30,\n",
        "        \"activation_function\":run.act_func,\n",
        "        \"Optimizer\":run.opt,\n",
        "        \"Regularisation\":run.reg\n",
        "        }\n",
        "    )\n",
        "    #images, labels = next(iter(self.trainloader))\n",
        "    #images, labels = images.to(device), labels.to(device)\n",
        "    #grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "    #self.tb.add_image('images', grid)\n",
        "    #self.tb.add_graph(self.network, images)\n",
        "\n",
        "  # when run ends, close TensorBoard, zero epoch count\n",
        "  def end_run(self):\n",
        "    wandb.finish()\n",
        "    self.tb.close()\n",
        "    self.epoch_count = 0\n",
        "\n",
        "  # zero epoch count, loss, accuracy, \n",
        "  def begin_epoch(self):\n",
        "    self.epoch_start_time = time.time()\n",
        "\n",
        "    self.epoch_count += 1\n",
        "    self.epoch_loss = 0\n",
        "    self.epoch_val_loss = 0\n",
        "    self.epoch_num_correct_train = 0\n",
        "    self.epoch_num_correct_test = 0\n",
        "    print(\"Training Epoch\",self.epoch_count)\n",
        "\n",
        "  # \n",
        "  def end_epoch(self):\n",
        "    # calculate epoch duration and run duration(accumulate)\n",
        "    epoch_duration = time.time() - self.epoch_start_time\n",
        "    run_duration = time.time() - self.run_start_time\n",
        "\n",
        "    # record epoch loss and accuracy\n",
        "    train_loss = self.epoch_loss / len(self.trainloader.dataset)\n",
        "    val_loss = self.epoch_val_loss / len(self.valloader.dataset)\n",
        "    train_accuracy = self.epoch_num_correct_train / len(self.trainloader.dataset)\n",
        "    test_accuracy = self.epoch_num_correct_test / len(self.valloader.dataset)\n",
        "    # Record epoch loss and accuracy to TensorBoard \n",
        "    self.tb.add_scalar('Train Loss', train_loss, self.epoch_count)\n",
        "    self.tb.add_scalar('train Accuracy', train_accuracy, self.epoch_count)\n",
        "    self.tb.add_scalar('Validation Loss', val_loss, self.epoch_count)\n",
        "    self.tb.add_scalar('Validation Accuracy', test_accuracy, self.epoch_count)\n",
        "\n",
        "    # Record params to TensorBoard\n",
        "    #for name, param in self.network.named_parameters():\n",
        "      #self.tb.add_histogram(name, param, self.epoch_count)\n",
        "      #self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
        "    \n",
        "    # Write into 'results' (OrderedDict) for all run related data\n",
        "    results = OrderedDict()\n",
        "    results[\"run\"] = self.run_count\n",
        "    results[\"epoch\"] = self.epoch_count\n",
        "    results[\"train_loss\"] = train_loss\n",
        "    results[\"val_loss\"] = val_loss\n",
        "    results[\"train_accuracy\"] = train_accuracy\n",
        "    results[\"val_accuracy\"] = test_accuracy\n",
        "    results[\"epoch duration\"] = epoch_duration\n",
        "    results[\"run duration\"] = run_duration\n",
        "    print(\"train_acc = \", train_accuracy, \"train_loss = \", train_loss, \"Validation_acc = \", test_accuracy, \"Validation_loss = \", val_loss)\n",
        "    wandb.log({\"train_acc\": train_accuracy, \"train_loss\": train_loss, \"Validation_acc\": test_accuracy, \"Validation_loss\": val_loss})\n",
        "    \n",
        "    # Record hyper-params into 'results'\n",
        "    for k,v in self.run_params._asdict().items(): results[k] = v\n",
        "    self.run_data.append(results)\n",
        "    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')\n",
        "\n",
        "    # display epoch information and show progress\n",
        "    # clear_output(wait=True)\n",
        "    # display(df)\n",
        "\n",
        "  # accumulate loss of batch into entire epoch loss\n",
        "  def track_loss(self, loss):\n",
        "    # multiply batch size so variety of batch sizes can be compared\n",
        "    self.epoch_loss += loss.item() * self.trainloader.batch_size\n",
        "  def track_val_loss(self, loss):\n",
        "    # multiply batch size so variety of batch sizes can be compared\n",
        "    self.epoch_val_loss += loss.item() * self.valloader.batch_size\n",
        "  # accumulate number of corrects of batch into entire epoch num_correct\n",
        "  def track_num_correct_train(self, preds, labels):\n",
        "    self.epoch_num_correct_train += self._get_num_correct(preds, labels)\n",
        "  def track_num_correct_test(self, preds, labels):\n",
        "    self.epoch_num_correct_test += self._get_num_correct(preds, labels)\n",
        "  @torch.no_grad()\n",
        "  def _get_num_correct(self, preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "  \n",
        "  # save end results of all runs into csv, json for further a\n",
        "  def save(self, fileName):\n",
        "\n",
        "    pd.DataFrame.from_dict(\n",
        "        self.run_data, \n",
        "        orient = 'columns',\n",
        "    ).to_csv(f'{fileName}.csv')\n",
        "\n",
        "    with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Shake Drop Regularisation\n"
      ],
      "metadata": {
        "id": "74WW3tEuOpjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cl7bmnHfA8H8"
      },
      "outputs": [],
      "source": [
        "class ShakeDropFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, training=True, p_drop=0.5, alpha_range=[-1, 1]):\n",
        "        if training:\n",
        "            gate = torch.cuda.FloatTensor([0]).bernoulli_(1 - p_drop)\n",
        "            ctx.save_for_backward(gate)\n",
        "            if gate.item() == 0:\n",
        "                alpha = torch.cuda.FloatTensor(x.size(0)).uniform_(*alpha_range)\n",
        "                alpha = alpha.view(alpha.size(0), 1, 1, 1).expand_as(x)\n",
        "                return alpha * x\n",
        "            else:\n",
        "                return x\n",
        "        else:\n",
        "            return (1 - p_drop) * x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        gate = ctx.saved_tensors[0]\n",
        "        if gate.item() == 0:\n",
        "            beta = torch.cuda.FloatTensor(grad_output.size(0)).uniform_(0, 1)\n",
        "            beta = beta.view(beta.size(0), 1, 1, 1).expand_as(grad_output)\n",
        "            beta = Variable(beta)\n",
        "            return beta * grad_output, None, None, None\n",
        "        else:\n",
        "            return grad_output, None, None, None\n",
        "\n",
        "\n",
        "class ShakeDrop(nn.Module):\n",
        "\n",
        "    def __init__(self, p_drop=0.5, alpha_range=[-1, 1]):\n",
        "        super(ShakeDrop, self).__init__()\n",
        "        self.p_drop = p_drop\n",
        "        self.alpha_range = alpha_range\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ShakeDropFunction.apply(x, self.training, self.p_drop, self.alpha_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Network as per given Instructions"
      ],
      "metadata": {
        "id": "I7DYL7SCeMWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nothing(x):\n",
        "  return x"
      ],
      "metadata": {
        "id": "yLIzos6Fj4JY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    outchannel_ratio = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, p_shakedrop = 1.0, reg='no_reg'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        if reg == 'shake-drop':\n",
        "            self.drop = ShakeDrop(p_shakedrop)\n",
        "        elif reg == 'dropout':\n",
        "            self.drop = nn.Dropout(p=p_shakedrop)\n",
        "        else:\n",
        "            self.drop = nothing\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.bn1(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.drop(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "            featuremap_size = shortcut.size()[2:4]\n",
        "        else:\n",
        "            shortcut = x\n",
        "            featuremap_size = out.size()[2:4]\n",
        "\n",
        "        batch_size = out.size()[0]\n",
        "        residual_channel = out.size()[1]\n",
        "        shortcut_channel = shortcut.size()[1]\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        if residual_channel != shortcut_channel:\n",
        "            padding = torch.autograd.Variable(\n",
        "                torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0],\n",
        "                                       featuremap_size[1]).fill_(0))\n",
        "            out = out + torch.cat((shortcut, padding), 1)\n",
        "        else:\n",
        "            out = out + shortcut\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    outchannel_ratio = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,p_shakedrop=1.0, reg='no_reg'):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, (planes * 1), kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d((planes * 1))\n",
        "        self.conv3 = nn.Conv2d((planes * 1), planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(planes * Bottleneck.outchannel_ratio)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        if reg == 'shake-drop':\n",
        "            self.drop = ShakeDrop(p_shakedrop)\n",
        "        elif reg == 'dropout':\n",
        "            self.drop = nn.Dropout(p=p_shakedrop)\n",
        "        else:\n",
        "            self.drop = nothing\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        out = self.bn4(out)\n",
        "\n",
        "        out = self.drop(out)\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "            featuremap_size = shortcut.size()[2:4]\n",
        "        else:\n",
        "            shortcut = x\n",
        "            featuremap_size = out.size()[2:4]\n",
        "\n",
        "        batch_size = out.size()[0]\n",
        "        residual_channel = out.size()[1]\n",
        "        shortcut_channel = shortcut.size()[1]\n",
        "\n",
        "        if residual_channel != shortcut_channel:\n",
        "            padding = torch.autograd.Variable(\n",
        "                torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0],\n",
        "                                       featuremap_size[1]).fill_(0))\n",
        "            out += torch.cat((shortcut, padding), 1)\n",
        "        else:\n",
        "            out += shortcut\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidNet_ShakeDrop(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, alpha, num_classes, bottleneck=False, reg='no_reg'):\n",
        "        super(PyramidNet_ShakeDrop, self).__init__()\n",
        "        blocks = {18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
        "        layers_list = {18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3],\n",
        "                  200: [3, 24, 36, 3]}\n",
        "\n",
        "        if layers_list.get(depth) is None:\n",
        "            if bottleneck == True:\n",
        "                blocks[depth] = Bottleneck\n",
        "                temp_cfg = int((depth - 2) / 12)\n",
        "            else:\n",
        "                blocks[depth] = BasicBlock\n",
        "                temp_cfg = int((depth - 2) / 8)\n",
        "\n",
        "            layers_list[depth] = [temp_cfg, temp_cfg, temp_cfg, temp_cfg]\n",
        "            print('=> the layer configuration for each stage is set to', layers_list[depth])\n",
        "\n",
        "        # self.u_idx is the index of self.p_drop\n",
        "        # self.p_drop is initialized to an geometric sequence, also can refer to the parameter setting method in the paper\n",
        "        self.u_idx = 0\n",
        "        all_depth = sum(layers_list[depth])\n",
        "        self.p_drop = [0.5/all_depth * (i + 1) for i in range(all_depth)]\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.addrate = alpha / (sum(layers_list[depth]) * 1.0)\n",
        "\n",
        "        self.input_featuremap_dim = self.inplanes\n",
        "        # down1\n",
        "        self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # down2\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.featuremap_dim = self.input_featuremap_dim\n",
        "        self.layer1 = self.pyramidal_make_layer(blocks[depth], layers_list[depth][0],reg=reg)\n",
        "        # down 3,4,5\n",
        "        self.layer2 = self.pyramidal_make_layer(blocks[depth], layers_list[depth][1], stride=2, reg=reg)\n",
        "        self.layer3 = self.pyramidal_make_layer(blocks[depth], layers_list[depth][2], stride=2, reg=reg)\n",
        "        self.layer4 = self.pyramidal_make_layer(blocks[depth], layers_list[depth][3], stride=2, reg=reg)\n",
        "\n",
        "        self.final_featuremap_dim = self.input_featuremap_dim\n",
        "        self.bn_final = nn.BatchNorm2d(self.final_featuremap_dim)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def pyramidal_make_layer(self, block, block_depth, stride=1, reg='no_reg'):\n",
        "        downsample = None\n",
        "        if stride != 1:  # or self.inplanes != int(round(featuremap_dim_1st)) * block.outchannel_ratio:\n",
        "            downsample = nn.AvgPool2d((2, 2), stride=(2, 2), ceil_mode=True)\n",
        "\n",
        "        layers = []\n",
        "        self.featuremap_dim = self.featuremap_dim + self.addrate\n",
        "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample,self.p_drop[self.u_idx],reg=reg))\n",
        "        self.u_idx += 1\n",
        "        for i in range(1, block_depth):\n",
        "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
        "            layers.append(\n",
        "                block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1, None,self.p_drop[self.u_idx]))\n",
        "            self.u_idx += 1\n",
        "            self.featuremap_dim = temp_featuremap_dim\n",
        "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.bn_final(x)\n",
        "        x = self.relu_final(x)\n",
        "        # x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PJgnbwgeCgHG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PyramidNet_ShakeDrop(depth=110, alpha=270, num_classes=10, reg='shake-drop').to(device)\n",
        "print(summary(model, ( 3, 32, 32)))\n",
        "model = PyramidNet_ShakeDrop(depth=110, alpha=270, num_classes=10, reg='dropout').to(device)\n",
        "print(summary(model, ( 3, 32, 32)))\n",
        "model = PyramidNet_ShakeDrop(depth=110, alpha=270, num_classes=10, reg='no_reg').to(device)\n",
        "print(summary(model, ( 3, 32, 32)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Q71Z5rPcVh",
        "outputId": "21e468da-d95d-49bc-a80e-3d61c28b3d12"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> the layer configuration for each stage is set to [13, 13, 13, 13]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
            "            Conv2d-6             [-1, 69, 8, 8]          39,744\n",
            "       BatchNorm2d-7             [-1, 69, 8, 8]             138\n",
            "              ReLU-8             [-1, 69, 8, 8]               0\n",
            "            Conv2d-9             [-1, 69, 8, 8]          42,849\n",
            "      BatchNorm2d-10             [-1, 69, 8, 8]             138\n",
            "        ShakeDrop-11             [-1, 69, 8, 8]               0\n",
            "       BasicBlock-12             [-1, 69, 8, 8]               0\n",
            "      BatchNorm2d-13             [-1, 69, 8, 8]             138\n",
            "           Conv2d-14             [-1, 74, 8, 8]          45,954\n",
            "      BatchNorm2d-15             [-1, 74, 8, 8]             148\n",
            "             ReLU-16             [-1, 74, 8, 8]               0\n",
            "           Conv2d-17             [-1, 74, 8, 8]          49,284\n",
            "      BatchNorm2d-18             [-1, 74, 8, 8]             148\n",
            "       BasicBlock-19             [-1, 74, 8, 8]               0\n",
            "      BatchNorm2d-20             [-1, 74, 8, 8]             148\n",
            "           Conv2d-21             [-1, 80, 8, 8]          53,280\n",
            "      BatchNorm2d-22             [-1, 80, 8, 8]             160\n",
            "             ReLU-23             [-1, 80, 8, 8]               0\n",
            "           Conv2d-24             [-1, 80, 8, 8]          57,600\n",
            "      BatchNorm2d-25             [-1, 80, 8, 8]             160\n",
            "       BasicBlock-26             [-1, 80, 8, 8]               0\n",
            "      BatchNorm2d-27             [-1, 80, 8, 8]             160\n",
            "           Conv2d-28             [-1, 85, 8, 8]          61,200\n",
            "      BatchNorm2d-29             [-1, 85, 8, 8]             170\n",
            "             ReLU-30             [-1, 85, 8, 8]               0\n",
            "           Conv2d-31             [-1, 85, 8, 8]          65,025\n",
            "      BatchNorm2d-32             [-1, 85, 8, 8]             170\n",
            "       BasicBlock-33             [-1, 85, 8, 8]               0\n",
            "      BatchNorm2d-34             [-1, 85, 8, 8]             170\n",
            "           Conv2d-35             [-1, 90, 8, 8]          68,850\n",
            "      BatchNorm2d-36             [-1, 90, 8, 8]             180\n",
            "             ReLU-37             [-1, 90, 8, 8]               0\n",
            "           Conv2d-38             [-1, 90, 8, 8]          72,900\n",
            "      BatchNorm2d-39             [-1, 90, 8, 8]             180\n",
            "       BasicBlock-40             [-1, 90, 8, 8]               0\n",
            "      BatchNorm2d-41             [-1, 90, 8, 8]             180\n",
            "           Conv2d-42             [-1, 95, 8, 8]          76,950\n",
            "      BatchNorm2d-43             [-1, 95, 8, 8]             190\n",
            "             ReLU-44             [-1, 95, 8, 8]               0\n",
            "           Conv2d-45             [-1, 95, 8, 8]          81,225\n",
            "      BatchNorm2d-46             [-1, 95, 8, 8]             190\n",
            "       BasicBlock-47             [-1, 95, 8, 8]               0\n",
            "      BatchNorm2d-48             [-1, 95, 8, 8]             190\n",
            "           Conv2d-49            [-1, 100, 8, 8]          85,500\n",
            "      BatchNorm2d-50            [-1, 100, 8, 8]             200\n",
            "             ReLU-51            [-1, 100, 8, 8]               0\n",
            "           Conv2d-52            [-1, 100, 8, 8]          90,000\n",
            "      BatchNorm2d-53            [-1, 100, 8, 8]             200\n",
            "       BasicBlock-54            [-1, 100, 8, 8]               0\n",
            "      BatchNorm2d-55            [-1, 100, 8, 8]             200\n",
            "           Conv2d-56            [-1, 106, 8, 8]          95,400\n",
            "      BatchNorm2d-57            [-1, 106, 8, 8]             212\n",
            "             ReLU-58            [-1, 106, 8, 8]               0\n",
            "           Conv2d-59            [-1, 106, 8, 8]         101,124\n",
            "      BatchNorm2d-60            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-61            [-1, 106, 8, 8]               0\n",
            "      BatchNorm2d-62            [-1, 106, 8, 8]             212\n",
            "           Conv2d-63            [-1, 111, 8, 8]         105,894\n",
            "      BatchNorm2d-64            [-1, 111, 8, 8]             222\n",
            "             ReLU-65            [-1, 111, 8, 8]               0\n",
            "           Conv2d-66            [-1, 111, 8, 8]         110,889\n",
            "      BatchNorm2d-67            [-1, 111, 8, 8]             222\n",
            "       BasicBlock-68            [-1, 111, 8, 8]               0\n",
            "      BatchNorm2d-69            [-1, 111, 8, 8]             222\n",
            "           Conv2d-70            [-1, 116, 8, 8]         115,884\n",
            "      BatchNorm2d-71            [-1, 116, 8, 8]             232\n",
            "             ReLU-72            [-1, 116, 8, 8]               0\n",
            "           Conv2d-73            [-1, 116, 8, 8]         121,104\n",
            "      BatchNorm2d-74            [-1, 116, 8, 8]             232\n",
            "       BasicBlock-75            [-1, 116, 8, 8]               0\n",
            "      BatchNorm2d-76            [-1, 116, 8, 8]             232\n",
            "           Conv2d-77            [-1, 121, 8, 8]         126,324\n",
            "      BatchNorm2d-78            [-1, 121, 8, 8]             242\n",
            "             ReLU-79            [-1, 121, 8, 8]               0\n",
            "           Conv2d-80            [-1, 121, 8, 8]         131,769\n",
            "      BatchNorm2d-81            [-1, 121, 8, 8]             242\n",
            "       BasicBlock-82            [-1, 121, 8, 8]               0\n",
            "      BatchNorm2d-83            [-1, 121, 8, 8]             242\n",
            "           Conv2d-84            [-1, 126, 8, 8]         137,214\n",
            "      BatchNorm2d-85            [-1, 126, 8, 8]             252\n",
            "             ReLU-86            [-1, 126, 8, 8]               0\n",
            "           Conv2d-87            [-1, 126, 8, 8]         142,884\n",
            "      BatchNorm2d-88            [-1, 126, 8, 8]             252\n",
            "       BasicBlock-89            [-1, 126, 8, 8]               0\n",
            "      BatchNorm2d-90            [-1, 126, 8, 8]             252\n",
            "           Conv2d-91            [-1, 132, 8, 8]         149,688\n",
            "      BatchNorm2d-92            [-1, 132, 8, 8]             264\n",
            "             ReLU-93            [-1, 132, 8, 8]               0\n",
            "           Conv2d-94            [-1, 132, 8, 8]         156,816\n",
            "      BatchNorm2d-95            [-1, 132, 8, 8]             264\n",
            "       BasicBlock-96            [-1, 132, 8, 8]               0\n",
            "      BatchNorm2d-97            [-1, 132, 8, 8]             264\n",
            "           Conv2d-98            [-1, 137, 4, 4]         162,756\n",
            "      BatchNorm2d-99            [-1, 137, 4, 4]             274\n",
            "            ReLU-100            [-1, 137, 4, 4]               0\n",
            "          Conv2d-101            [-1, 137, 4, 4]         168,921\n",
            "     BatchNorm2d-102            [-1, 137, 4, 4]             274\n",
            "       ShakeDrop-103            [-1, 137, 4, 4]               0\n",
            "       AvgPool2d-104            [-1, 132, 4, 4]               0\n",
            "      BasicBlock-105            [-1, 137, 4, 4]               0\n",
            "     BatchNorm2d-106            [-1, 137, 4, 4]             274\n",
            "          Conv2d-107            [-1, 142, 4, 4]         175,086\n",
            "     BatchNorm2d-108            [-1, 142, 4, 4]             284\n",
            "            ReLU-109            [-1, 142, 4, 4]               0\n",
            "          Conv2d-110            [-1, 142, 4, 4]         181,476\n",
            "     BatchNorm2d-111            [-1, 142, 4, 4]             284\n",
            "      BasicBlock-112            [-1, 142, 4, 4]               0\n",
            "     BatchNorm2d-113            [-1, 142, 4, 4]             284\n",
            "          Conv2d-114            [-1, 147, 4, 4]         187,866\n",
            "     BatchNorm2d-115            [-1, 147, 4, 4]             294\n",
            "            ReLU-116            [-1, 147, 4, 4]               0\n",
            "          Conv2d-117            [-1, 147, 4, 4]         194,481\n",
            "     BatchNorm2d-118            [-1, 147, 4, 4]             294\n",
            "      BasicBlock-119            [-1, 147, 4, 4]               0\n",
            "     BatchNorm2d-120            [-1, 147, 4, 4]             294\n",
            "          Conv2d-121            [-1, 152, 4, 4]         201,096\n",
            "     BatchNorm2d-122            [-1, 152, 4, 4]             304\n",
            "            ReLU-123            [-1, 152, 4, 4]               0\n",
            "          Conv2d-124            [-1, 152, 4, 4]         207,936\n",
            "     BatchNorm2d-125            [-1, 152, 4, 4]             304\n",
            "      BasicBlock-126            [-1, 152, 4, 4]               0\n",
            "     BatchNorm2d-127            [-1, 152, 4, 4]             304\n",
            "          Conv2d-128            [-1, 157, 4, 4]         214,776\n",
            "     BatchNorm2d-129            [-1, 157, 4, 4]             314\n",
            "            ReLU-130            [-1, 157, 4, 4]               0\n",
            "          Conv2d-131            [-1, 157, 4, 4]         221,841\n",
            "     BatchNorm2d-132            [-1, 157, 4, 4]             314\n",
            "      BasicBlock-133            [-1, 157, 4, 4]               0\n",
            "     BatchNorm2d-134            [-1, 157, 4, 4]             314\n",
            "          Conv2d-135            [-1, 163, 4, 4]         230,319\n",
            "     BatchNorm2d-136            [-1, 163, 4, 4]             326\n",
            "            ReLU-137            [-1, 163, 4, 4]               0\n",
            "          Conv2d-138            [-1, 163, 4, 4]         239,121\n",
            "     BatchNorm2d-139            [-1, 163, 4, 4]             326\n",
            "      BasicBlock-140            [-1, 163, 4, 4]               0\n",
            "     BatchNorm2d-141            [-1, 163, 4, 4]             326\n",
            "          Conv2d-142            [-1, 168, 4, 4]         246,456\n",
            "     BatchNorm2d-143            [-1, 168, 4, 4]             336\n",
            "            ReLU-144            [-1, 168, 4, 4]               0\n",
            "          Conv2d-145            [-1, 168, 4, 4]         254,016\n",
            "     BatchNorm2d-146            [-1, 168, 4, 4]             336\n",
            "      BasicBlock-147            [-1, 168, 4, 4]               0\n",
            "     BatchNorm2d-148            [-1, 168, 4, 4]             336\n",
            "          Conv2d-149            [-1, 173, 4, 4]         261,576\n",
            "     BatchNorm2d-150            [-1, 173, 4, 4]             346\n",
            "            ReLU-151            [-1, 173, 4, 4]               0\n",
            "          Conv2d-152            [-1, 173, 4, 4]         269,361\n",
            "     BatchNorm2d-153            [-1, 173, 4, 4]             346\n",
            "      BasicBlock-154            [-1, 173, 4, 4]               0\n",
            "     BatchNorm2d-155            [-1, 173, 4, 4]             346\n",
            "          Conv2d-156            [-1, 178, 4, 4]         277,146\n",
            "     BatchNorm2d-157            [-1, 178, 4, 4]             356\n",
            "            ReLU-158            [-1, 178, 4, 4]               0\n",
            "          Conv2d-159            [-1, 178, 4, 4]         285,156\n",
            "     BatchNorm2d-160            [-1, 178, 4, 4]             356\n",
            "      BasicBlock-161            [-1, 178, 4, 4]               0\n",
            "     BatchNorm2d-162            [-1, 178, 4, 4]             356\n",
            "          Conv2d-163            [-1, 183, 4, 4]         293,166\n",
            "     BatchNorm2d-164            [-1, 183, 4, 4]             366\n",
            "            ReLU-165            [-1, 183, 4, 4]               0\n",
            "          Conv2d-166            [-1, 183, 4, 4]         301,401\n",
            "     BatchNorm2d-167            [-1, 183, 4, 4]             366\n",
            "      BasicBlock-168            [-1, 183, 4, 4]               0\n",
            "     BatchNorm2d-169            [-1, 183, 4, 4]             366\n",
            "          Conv2d-170            [-1, 189, 4, 4]         311,283\n",
            "     BatchNorm2d-171            [-1, 189, 4, 4]             378\n",
            "            ReLU-172            [-1, 189, 4, 4]               0\n",
            "          Conv2d-173            [-1, 189, 4, 4]         321,489\n",
            "     BatchNorm2d-174            [-1, 189, 4, 4]             378\n",
            "      BasicBlock-175            [-1, 189, 4, 4]               0\n",
            "     BatchNorm2d-176            [-1, 189, 4, 4]             378\n",
            "          Conv2d-177            [-1, 194, 4, 4]         329,994\n",
            "     BatchNorm2d-178            [-1, 194, 4, 4]             388\n",
            "            ReLU-179            [-1, 194, 4, 4]               0\n",
            "          Conv2d-180            [-1, 194, 4, 4]         338,724\n",
            "     BatchNorm2d-181            [-1, 194, 4, 4]             388\n",
            "      BasicBlock-182            [-1, 194, 4, 4]               0\n",
            "     BatchNorm2d-183            [-1, 194, 4, 4]             388\n",
            "          Conv2d-184            [-1, 199, 4, 4]         347,454\n",
            "     BatchNorm2d-185            [-1, 199, 4, 4]             398\n",
            "            ReLU-186            [-1, 199, 4, 4]               0\n",
            "          Conv2d-187            [-1, 199, 4, 4]         356,409\n",
            "     BatchNorm2d-188            [-1, 199, 4, 4]             398\n",
            "      BasicBlock-189            [-1, 199, 4, 4]               0\n",
            "     BatchNorm2d-190            [-1, 199, 4, 4]             398\n",
            "          Conv2d-191            [-1, 204, 2, 2]         365,364\n",
            "     BatchNorm2d-192            [-1, 204, 2, 2]             408\n",
            "            ReLU-193            [-1, 204, 2, 2]               0\n",
            "          Conv2d-194            [-1, 204, 2, 2]         374,544\n",
            "     BatchNorm2d-195            [-1, 204, 2, 2]             408\n",
            "       ShakeDrop-196            [-1, 204, 2, 2]               0\n",
            "       AvgPool2d-197            [-1, 199, 2, 2]               0\n",
            "      BasicBlock-198            [-1, 204, 2, 2]               0\n",
            "     BatchNorm2d-199            [-1, 204, 2, 2]             408\n",
            "          Conv2d-200            [-1, 209, 2, 2]         383,724\n",
            "     BatchNorm2d-201            [-1, 209, 2, 2]             418\n",
            "            ReLU-202            [-1, 209, 2, 2]               0\n",
            "          Conv2d-203            [-1, 209, 2, 2]         393,129\n",
            "     BatchNorm2d-204            [-1, 209, 2, 2]             418\n",
            "      BasicBlock-205            [-1, 209, 2, 2]               0\n",
            "     BatchNorm2d-206            [-1, 209, 2, 2]             418\n",
            "          Conv2d-207            [-1, 215, 2, 2]         404,415\n",
            "     BatchNorm2d-208            [-1, 215, 2, 2]             430\n",
            "            ReLU-209            [-1, 215, 2, 2]               0\n",
            "          Conv2d-210            [-1, 215, 2, 2]         416,025\n",
            "     BatchNorm2d-211            [-1, 215, 2, 2]             430\n",
            "      BasicBlock-212            [-1, 215, 2, 2]               0\n",
            "     BatchNorm2d-213            [-1, 215, 2, 2]             430\n",
            "          Conv2d-214            [-1, 220, 2, 2]         425,700\n",
            "     BatchNorm2d-215            [-1, 220, 2, 2]             440\n",
            "            ReLU-216            [-1, 220, 2, 2]               0\n",
            "          Conv2d-217            [-1, 220, 2, 2]         435,600\n",
            "     BatchNorm2d-218            [-1, 220, 2, 2]             440\n",
            "      BasicBlock-219            [-1, 220, 2, 2]               0\n",
            "     BatchNorm2d-220            [-1, 220, 2, 2]             440\n",
            "          Conv2d-221            [-1, 225, 2, 2]         445,500\n",
            "     BatchNorm2d-222            [-1, 225, 2, 2]             450\n",
            "            ReLU-223            [-1, 225, 2, 2]               0\n",
            "          Conv2d-224            [-1, 225, 2, 2]         455,625\n",
            "     BatchNorm2d-225            [-1, 225, 2, 2]             450\n",
            "      BasicBlock-226            [-1, 225, 2, 2]               0\n",
            "     BatchNorm2d-227            [-1, 225, 2, 2]             450\n",
            "          Conv2d-228            [-1, 230, 2, 2]         465,750\n",
            "     BatchNorm2d-229            [-1, 230, 2, 2]             460\n",
            "            ReLU-230            [-1, 230, 2, 2]               0\n",
            "          Conv2d-231            [-1, 230, 2, 2]         476,100\n",
            "     BatchNorm2d-232            [-1, 230, 2, 2]             460\n",
            "      BasicBlock-233            [-1, 230, 2, 2]               0\n",
            "     BatchNorm2d-234            [-1, 230, 2, 2]             460\n",
            "          Conv2d-235            [-1, 235, 2, 2]         486,450\n",
            "     BatchNorm2d-236            [-1, 235, 2, 2]             470\n",
            "            ReLU-237            [-1, 235, 2, 2]               0\n",
            "          Conv2d-238            [-1, 235, 2, 2]         497,025\n",
            "     BatchNorm2d-239            [-1, 235, 2, 2]             470\n",
            "      BasicBlock-240            [-1, 235, 2, 2]               0\n",
            "     BatchNorm2d-241            [-1, 235, 2, 2]             470\n",
            "          Conv2d-242            [-1, 241, 2, 2]         509,715\n",
            "     BatchNorm2d-243            [-1, 241, 2, 2]             482\n",
            "            ReLU-244            [-1, 241, 2, 2]               0\n",
            "          Conv2d-245            [-1, 241, 2, 2]         522,729\n",
            "     BatchNorm2d-246            [-1, 241, 2, 2]             482\n",
            "      BasicBlock-247            [-1, 241, 2, 2]               0\n",
            "     BatchNorm2d-248            [-1, 241, 2, 2]             482\n",
            "          Conv2d-249            [-1, 246, 2, 2]         533,574\n",
            "     BatchNorm2d-250            [-1, 246, 2, 2]             492\n",
            "            ReLU-251            [-1, 246, 2, 2]               0\n",
            "          Conv2d-252            [-1, 246, 2, 2]         544,644\n",
            "     BatchNorm2d-253            [-1, 246, 2, 2]             492\n",
            "      BasicBlock-254            [-1, 246, 2, 2]               0\n",
            "     BatchNorm2d-255            [-1, 246, 2, 2]             492\n",
            "          Conv2d-256            [-1, 251, 2, 2]         555,714\n",
            "     BatchNorm2d-257            [-1, 251, 2, 2]             502\n",
            "            ReLU-258            [-1, 251, 2, 2]               0\n",
            "          Conv2d-259            [-1, 251, 2, 2]         567,009\n",
            "     BatchNorm2d-260            [-1, 251, 2, 2]             502\n",
            "      BasicBlock-261            [-1, 251, 2, 2]               0\n",
            "     BatchNorm2d-262            [-1, 251, 2, 2]             502\n",
            "          Conv2d-263            [-1, 256, 2, 2]         578,304\n",
            "     BatchNorm2d-264            [-1, 256, 2, 2]             512\n",
            "            ReLU-265            [-1, 256, 2, 2]               0\n",
            "          Conv2d-266            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-267            [-1, 256, 2, 2]             512\n",
            "      BasicBlock-268            [-1, 256, 2, 2]               0\n",
            "     BatchNorm2d-269            [-1, 256, 2, 2]             512\n",
            "          Conv2d-270            [-1, 261, 2, 2]         601,344\n",
            "     BatchNorm2d-271            [-1, 261, 2, 2]             522\n",
            "            ReLU-272            [-1, 261, 2, 2]               0\n",
            "          Conv2d-273            [-1, 261, 2, 2]         613,089\n",
            "     BatchNorm2d-274            [-1, 261, 2, 2]             522\n",
            "      BasicBlock-275            [-1, 261, 2, 2]               0\n",
            "     BatchNorm2d-276            [-1, 261, 2, 2]             522\n",
            "          Conv2d-277            [-1, 266, 2, 2]         624,834\n",
            "     BatchNorm2d-278            [-1, 266, 2, 2]             532\n",
            "            ReLU-279            [-1, 266, 2, 2]               0\n",
            "          Conv2d-280            [-1, 266, 2, 2]         636,804\n",
            "     BatchNorm2d-281            [-1, 266, 2, 2]             532\n",
            "      BasicBlock-282            [-1, 266, 2, 2]               0\n",
            "     BatchNorm2d-283            [-1, 266, 2, 2]             532\n",
            "          Conv2d-284            [-1, 272, 1, 1]         651,168\n",
            "     BatchNorm2d-285            [-1, 272, 1, 1]             544\n",
            "            ReLU-286            [-1, 272, 1, 1]               0\n",
            "          Conv2d-287            [-1, 272, 1, 1]         665,856\n",
            "     BatchNorm2d-288            [-1, 272, 1, 1]             544\n",
            "       ShakeDrop-289            [-1, 272, 1, 1]               0\n",
            "       AvgPool2d-290            [-1, 266, 1, 1]               0\n",
            "      BasicBlock-291            [-1, 272, 1, 1]               0\n",
            "     BatchNorm2d-292            [-1, 272, 1, 1]             544\n",
            "          Conv2d-293            [-1, 277, 1, 1]         678,096\n",
            "     BatchNorm2d-294            [-1, 277, 1, 1]             554\n",
            "            ReLU-295            [-1, 277, 1, 1]               0\n",
            "          Conv2d-296            [-1, 277, 1, 1]         690,561\n",
            "     BatchNorm2d-297            [-1, 277, 1, 1]             554\n",
            "      BasicBlock-298            [-1, 277, 1, 1]               0\n",
            "     BatchNorm2d-299            [-1, 277, 1, 1]             554\n",
            "          Conv2d-300            [-1, 282, 1, 1]         703,026\n",
            "     BatchNorm2d-301            [-1, 282, 1, 1]             564\n",
            "            ReLU-302            [-1, 282, 1, 1]               0\n",
            "          Conv2d-303            [-1, 282, 1, 1]         715,716\n",
            "     BatchNorm2d-304            [-1, 282, 1, 1]             564\n",
            "      BasicBlock-305            [-1, 282, 1, 1]               0\n",
            "     BatchNorm2d-306            [-1, 282, 1, 1]             564\n",
            "          Conv2d-307            [-1, 287, 1, 1]         728,406\n",
            "     BatchNorm2d-308            [-1, 287, 1, 1]             574\n",
            "            ReLU-309            [-1, 287, 1, 1]               0\n",
            "          Conv2d-310            [-1, 287, 1, 1]         741,321\n",
            "     BatchNorm2d-311            [-1, 287, 1, 1]             574\n",
            "      BasicBlock-312            [-1, 287, 1, 1]               0\n",
            "     BatchNorm2d-313            [-1, 287, 1, 1]             574\n",
            "          Conv2d-314            [-1, 292, 1, 1]         754,236\n",
            "     BatchNorm2d-315            [-1, 292, 1, 1]             584\n",
            "            ReLU-316            [-1, 292, 1, 1]               0\n",
            "          Conv2d-317            [-1, 292, 1, 1]         767,376\n",
            "     BatchNorm2d-318            [-1, 292, 1, 1]             584\n",
            "      BasicBlock-319            [-1, 292, 1, 1]               0\n",
            "     BatchNorm2d-320            [-1, 292, 1, 1]             584\n",
            "          Conv2d-321            [-1, 298, 1, 1]         783,144\n",
            "     BatchNorm2d-322            [-1, 298, 1, 1]             596\n",
            "            ReLU-323            [-1, 298, 1, 1]               0\n",
            "          Conv2d-324            [-1, 298, 1, 1]         799,236\n",
            "     BatchNorm2d-325            [-1, 298, 1, 1]             596\n",
            "      BasicBlock-326            [-1, 298, 1, 1]               0\n",
            "     BatchNorm2d-327            [-1, 298, 1, 1]             596\n",
            "          Conv2d-328            [-1, 303, 1, 1]         812,646\n",
            "     BatchNorm2d-329            [-1, 303, 1, 1]             606\n",
            "            ReLU-330            [-1, 303, 1, 1]               0\n",
            "          Conv2d-331            [-1, 303, 1, 1]         826,281\n",
            "     BatchNorm2d-332            [-1, 303, 1, 1]             606\n",
            "      BasicBlock-333            [-1, 303, 1, 1]               0\n",
            "     BatchNorm2d-334            [-1, 303, 1, 1]             606\n",
            "          Conv2d-335            [-1, 308, 1, 1]         839,916\n",
            "     BatchNorm2d-336            [-1, 308, 1, 1]             616\n",
            "            ReLU-337            [-1, 308, 1, 1]               0\n",
            "          Conv2d-338            [-1, 308, 1, 1]         853,776\n",
            "     BatchNorm2d-339            [-1, 308, 1, 1]             616\n",
            "      BasicBlock-340            [-1, 308, 1, 1]               0\n",
            "     BatchNorm2d-341            [-1, 308, 1, 1]             616\n",
            "          Conv2d-342            [-1, 313, 1, 1]         867,636\n",
            "     BatchNorm2d-343            [-1, 313, 1, 1]             626\n",
            "            ReLU-344            [-1, 313, 1, 1]               0\n",
            "          Conv2d-345            [-1, 313, 1, 1]         881,721\n",
            "     BatchNorm2d-346            [-1, 313, 1, 1]             626\n",
            "      BasicBlock-347            [-1, 313, 1, 1]               0\n",
            "     BatchNorm2d-348            [-1, 313, 1, 1]             626\n",
            "          Conv2d-349            [-1, 318, 1, 1]         895,806\n",
            "     BatchNorm2d-350            [-1, 318, 1, 1]             636\n",
            "            ReLU-351            [-1, 318, 1, 1]               0\n",
            "          Conv2d-352            [-1, 318, 1, 1]         910,116\n",
            "     BatchNorm2d-353            [-1, 318, 1, 1]             636\n",
            "      BasicBlock-354            [-1, 318, 1, 1]               0\n",
            "     BatchNorm2d-355            [-1, 318, 1, 1]             636\n",
            "          Conv2d-356            [-1, 324, 1, 1]         927,288\n",
            "     BatchNorm2d-357            [-1, 324, 1, 1]             648\n",
            "            ReLU-358            [-1, 324, 1, 1]               0\n",
            "          Conv2d-359            [-1, 324, 1, 1]         944,784\n",
            "     BatchNorm2d-360            [-1, 324, 1, 1]             648\n",
            "      BasicBlock-361            [-1, 324, 1, 1]               0\n",
            "     BatchNorm2d-362            [-1, 324, 1, 1]             648\n",
            "          Conv2d-363            [-1, 329, 1, 1]         959,364\n",
            "     BatchNorm2d-364            [-1, 329, 1, 1]             658\n",
            "            ReLU-365            [-1, 329, 1, 1]               0\n",
            "          Conv2d-366            [-1, 329, 1, 1]         974,169\n",
            "     BatchNorm2d-367            [-1, 329, 1, 1]             658\n",
            "      BasicBlock-368            [-1, 329, 1, 1]               0\n",
            "     BatchNorm2d-369            [-1, 329, 1, 1]             658\n",
            "          Conv2d-370            [-1, 334, 1, 1]         988,974\n",
            "     BatchNorm2d-371            [-1, 334, 1, 1]             668\n",
            "            ReLU-372            [-1, 334, 1, 1]               0\n",
            "          Conv2d-373            [-1, 334, 1, 1]       1,004,004\n",
            "     BatchNorm2d-374            [-1, 334, 1, 1]             668\n",
            "      BasicBlock-375            [-1, 334, 1, 1]               0\n",
            "     BatchNorm2d-376            [-1, 334, 1, 1]             668\n",
            "            ReLU-377            [-1, 334, 1, 1]               0\n",
            "          Linear-378                   [-1, 10]           3,350\n",
            "================================================================\n",
            "Total params: 43,307,727\n",
            "Trainable params: 43,307,727\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.71\n",
            "Params size (MB): 165.21\n",
            "Estimated Total Size (MB): 172.93\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "=> the layer configuration for each stage is set to [13, 13, 13, 13]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
            "            Conv2d-6             [-1, 69, 8, 8]          39,744\n",
            "       BatchNorm2d-7             [-1, 69, 8, 8]             138\n",
            "              ReLU-8             [-1, 69, 8, 8]               0\n",
            "            Conv2d-9             [-1, 69, 8, 8]          42,849\n",
            "      BatchNorm2d-10             [-1, 69, 8, 8]             138\n",
            "          Dropout-11             [-1, 69, 8, 8]               0\n",
            "       BasicBlock-12             [-1, 69, 8, 8]               0\n",
            "      BatchNorm2d-13             [-1, 69, 8, 8]             138\n",
            "           Conv2d-14             [-1, 74, 8, 8]          45,954\n",
            "      BatchNorm2d-15             [-1, 74, 8, 8]             148\n",
            "             ReLU-16             [-1, 74, 8, 8]               0\n",
            "           Conv2d-17             [-1, 74, 8, 8]          49,284\n",
            "      BatchNorm2d-18             [-1, 74, 8, 8]             148\n",
            "       BasicBlock-19             [-1, 74, 8, 8]               0\n",
            "      BatchNorm2d-20             [-1, 74, 8, 8]             148\n",
            "           Conv2d-21             [-1, 80, 8, 8]          53,280\n",
            "      BatchNorm2d-22             [-1, 80, 8, 8]             160\n",
            "             ReLU-23             [-1, 80, 8, 8]               0\n",
            "           Conv2d-24             [-1, 80, 8, 8]          57,600\n",
            "      BatchNorm2d-25             [-1, 80, 8, 8]             160\n",
            "       BasicBlock-26             [-1, 80, 8, 8]               0\n",
            "      BatchNorm2d-27             [-1, 80, 8, 8]             160\n",
            "           Conv2d-28             [-1, 85, 8, 8]          61,200\n",
            "      BatchNorm2d-29             [-1, 85, 8, 8]             170\n",
            "             ReLU-30             [-1, 85, 8, 8]               0\n",
            "           Conv2d-31             [-1, 85, 8, 8]          65,025\n",
            "      BatchNorm2d-32             [-1, 85, 8, 8]             170\n",
            "       BasicBlock-33             [-1, 85, 8, 8]               0\n",
            "      BatchNorm2d-34             [-1, 85, 8, 8]             170\n",
            "           Conv2d-35             [-1, 90, 8, 8]          68,850\n",
            "      BatchNorm2d-36             [-1, 90, 8, 8]             180\n",
            "             ReLU-37             [-1, 90, 8, 8]               0\n",
            "           Conv2d-38             [-1, 90, 8, 8]          72,900\n",
            "      BatchNorm2d-39             [-1, 90, 8, 8]             180\n",
            "       BasicBlock-40             [-1, 90, 8, 8]               0\n",
            "      BatchNorm2d-41             [-1, 90, 8, 8]             180\n",
            "           Conv2d-42             [-1, 95, 8, 8]          76,950\n",
            "      BatchNorm2d-43             [-1, 95, 8, 8]             190\n",
            "             ReLU-44             [-1, 95, 8, 8]               0\n",
            "           Conv2d-45             [-1, 95, 8, 8]          81,225\n",
            "      BatchNorm2d-46             [-1, 95, 8, 8]             190\n",
            "       BasicBlock-47             [-1, 95, 8, 8]               0\n",
            "      BatchNorm2d-48             [-1, 95, 8, 8]             190\n",
            "           Conv2d-49            [-1, 100, 8, 8]          85,500\n",
            "      BatchNorm2d-50            [-1, 100, 8, 8]             200\n",
            "             ReLU-51            [-1, 100, 8, 8]               0\n",
            "           Conv2d-52            [-1, 100, 8, 8]          90,000\n",
            "      BatchNorm2d-53            [-1, 100, 8, 8]             200\n",
            "       BasicBlock-54            [-1, 100, 8, 8]               0\n",
            "      BatchNorm2d-55            [-1, 100, 8, 8]             200\n",
            "           Conv2d-56            [-1, 106, 8, 8]          95,400\n",
            "      BatchNorm2d-57            [-1, 106, 8, 8]             212\n",
            "             ReLU-58            [-1, 106, 8, 8]               0\n",
            "           Conv2d-59            [-1, 106, 8, 8]         101,124\n",
            "      BatchNorm2d-60            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-61            [-1, 106, 8, 8]               0\n",
            "      BatchNorm2d-62            [-1, 106, 8, 8]             212\n",
            "           Conv2d-63            [-1, 111, 8, 8]         105,894\n",
            "      BatchNorm2d-64            [-1, 111, 8, 8]             222\n",
            "             ReLU-65            [-1, 111, 8, 8]               0\n",
            "           Conv2d-66            [-1, 111, 8, 8]         110,889\n",
            "      BatchNorm2d-67            [-1, 111, 8, 8]             222\n",
            "       BasicBlock-68            [-1, 111, 8, 8]               0\n",
            "      BatchNorm2d-69            [-1, 111, 8, 8]             222\n",
            "           Conv2d-70            [-1, 116, 8, 8]         115,884\n",
            "      BatchNorm2d-71            [-1, 116, 8, 8]             232\n",
            "             ReLU-72            [-1, 116, 8, 8]               0\n",
            "           Conv2d-73            [-1, 116, 8, 8]         121,104\n",
            "      BatchNorm2d-74            [-1, 116, 8, 8]             232\n",
            "       BasicBlock-75            [-1, 116, 8, 8]               0\n",
            "      BatchNorm2d-76            [-1, 116, 8, 8]             232\n",
            "           Conv2d-77            [-1, 121, 8, 8]         126,324\n",
            "      BatchNorm2d-78            [-1, 121, 8, 8]             242\n",
            "             ReLU-79            [-1, 121, 8, 8]               0\n",
            "           Conv2d-80            [-1, 121, 8, 8]         131,769\n",
            "      BatchNorm2d-81            [-1, 121, 8, 8]             242\n",
            "       BasicBlock-82            [-1, 121, 8, 8]               0\n",
            "      BatchNorm2d-83            [-1, 121, 8, 8]             242\n",
            "           Conv2d-84            [-1, 126, 8, 8]         137,214\n",
            "      BatchNorm2d-85            [-1, 126, 8, 8]             252\n",
            "             ReLU-86            [-1, 126, 8, 8]               0\n",
            "           Conv2d-87            [-1, 126, 8, 8]         142,884\n",
            "      BatchNorm2d-88            [-1, 126, 8, 8]             252\n",
            "       BasicBlock-89            [-1, 126, 8, 8]               0\n",
            "      BatchNorm2d-90            [-1, 126, 8, 8]             252\n",
            "           Conv2d-91            [-1, 132, 8, 8]         149,688\n",
            "      BatchNorm2d-92            [-1, 132, 8, 8]             264\n",
            "             ReLU-93            [-1, 132, 8, 8]               0\n",
            "           Conv2d-94            [-1, 132, 8, 8]         156,816\n",
            "      BatchNorm2d-95            [-1, 132, 8, 8]             264\n",
            "       BasicBlock-96            [-1, 132, 8, 8]               0\n",
            "      BatchNorm2d-97            [-1, 132, 8, 8]             264\n",
            "           Conv2d-98            [-1, 137, 4, 4]         162,756\n",
            "      BatchNorm2d-99            [-1, 137, 4, 4]             274\n",
            "            ReLU-100            [-1, 137, 4, 4]               0\n",
            "          Conv2d-101            [-1, 137, 4, 4]         168,921\n",
            "     BatchNorm2d-102            [-1, 137, 4, 4]             274\n",
            "         Dropout-103            [-1, 137, 4, 4]               0\n",
            "       AvgPool2d-104            [-1, 132, 4, 4]               0\n",
            "      BasicBlock-105            [-1, 137, 4, 4]               0\n",
            "     BatchNorm2d-106            [-1, 137, 4, 4]             274\n",
            "          Conv2d-107            [-1, 142, 4, 4]         175,086\n",
            "     BatchNorm2d-108            [-1, 142, 4, 4]             284\n",
            "            ReLU-109            [-1, 142, 4, 4]               0\n",
            "          Conv2d-110            [-1, 142, 4, 4]         181,476\n",
            "     BatchNorm2d-111            [-1, 142, 4, 4]             284\n",
            "      BasicBlock-112            [-1, 142, 4, 4]               0\n",
            "     BatchNorm2d-113            [-1, 142, 4, 4]             284\n",
            "          Conv2d-114            [-1, 147, 4, 4]         187,866\n",
            "     BatchNorm2d-115            [-1, 147, 4, 4]             294\n",
            "            ReLU-116            [-1, 147, 4, 4]               0\n",
            "          Conv2d-117            [-1, 147, 4, 4]         194,481\n",
            "     BatchNorm2d-118            [-1, 147, 4, 4]             294\n",
            "      BasicBlock-119            [-1, 147, 4, 4]               0\n",
            "     BatchNorm2d-120            [-1, 147, 4, 4]             294\n",
            "          Conv2d-121            [-1, 152, 4, 4]         201,096\n",
            "     BatchNorm2d-122            [-1, 152, 4, 4]             304\n",
            "            ReLU-123            [-1, 152, 4, 4]               0\n",
            "          Conv2d-124            [-1, 152, 4, 4]         207,936\n",
            "     BatchNorm2d-125            [-1, 152, 4, 4]             304\n",
            "      BasicBlock-126            [-1, 152, 4, 4]               0\n",
            "     BatchNorm2d-127            [-1, 152, 4, 4]             304\n",
            "          Conv2d-128            [-1, 157, 4, 4]         214,776\n",
            "     BatchNorm2d-129            [-1, 157, 4, 4]             314\n",
            "            ReLU-130            [-1, 157, 4, 4]               0\n",
            "          Conv2d-131            [-1, 157, 4, 4]         221,841\n",
            "     BatchNorm2d-132            [-1, 157, 4, 4]             314\n",
            "      BasicBlock-133            [-1, 157, 4, 4]               0\n",
            "     BatchNorm2d-134            [-1, 157, 4, 4]             314\n",
            "          Conv2d-135            [-1, 163, 4, 4]         230,319\n",
            "     BatchNorm2d-136            [-1, 163, 4, 4]             326\n",
            "            ReLU-137            [-1, 163, 4, 4]               0\n",
            "          Conv2d-138            [-1, 163, 4, 4]         239,121\n",
            "     BatchNorm2d-139            [-1, 163, 4, 4]             326\n",
            "      BasicBlock-140            [-1, 163, 4, 4]               0\n",
            "     BatchNorm2d-141            [-1, 163, 4, 4]             326\n",
            "          Conv2d-142            [-1, 168, 4, 4]         246,456\n",
            "     BatchNorm2d-143            [-1, 168, 4, 4]             336\n",
            "            ReLU-144            [-1, 168, 4, 4]               0\n",
            "          Conv2d-145            [-1, 168, 4, 4]         254,016\n",
            "     BatchNorm2d-146            [-1, 168, 4, 4]             336\n",
            "      BasicBlock-147            [-1, 168, 4, 4]               0\n",
            "     BatchNorm2d-148            [-1, 168, 4, 4]             336\n",
            "          Conv2d-149            [-1, 173, 4, 4]         261,576\n",
            "     BatchNorm2d-150            [-1, 173, 4, 4]             346\n",
            "            ReLU-151            [-1, 173, 4, 4]               0\n",
            "          Conv2d-152            [-1, 173, 4, 4]         269,361\n",
            "     BatchNorm2d-153            [-1, 173, 4, 4]             346\n",
            "      BasicBlock-154            [-1, 173, 4, 4]               0\n",
            "     BatchNorm2d-155            [-1, 173, 4, 4]             346\n",
            "          Conv2d-156            [-1, 178, 4, 4]         277,146\n",
            "     BatchNorm2d-157            [-1, 178, 4, 4]             356\n",
            "            ReLU-158            [-1, 178, 4, 4]               0\n",
            "          Conv2d-159            [-1, 178, 4, 4]         285,156\n",
            "     BatchNorm2d-160            [-1, 178, 4, 4]             356\n",
            "      BasicBlock-161            [-1, 178, 4, 4]               0\n",
            "     BatchNorm2d-162            [-1, 178, 4, 4]             356\n",
            "          Conv2d-163            [-1, 183, 4, 4]         293,166\n",
            "     BatchNorm2d-164            [-1, 183, 4, 4]             366\n",
            "            ReLU-165            [-1, 183, 4, 4]               0\n",
            "          Conv2d-166            [-1, 183, 4, 4]         301,401\n",
            "     BatchNorm2d-167            [-1, 183, 4, 4]             366\n",
            "      BasicBlock-168            [-1, 183, 4, 4]               0\n",
            "     BatchNorm2d-169            [-1, 183, 4, 4]             366\n",
            "          Conv2d-170            [-1, 189, 4, 4]         311,283\n",
            "     BatchNorm2d-171            [-1, 189, 4, 4]             378\n",
            "            ReLU-172            [-1, 189, 4, 4]               0\n",
            "          Conv2d-173            [-1, 189, 4, 4]         321,489\n",
            "     BatchNorm2d-174            [-1, 189, 4, 4]             378\n",
            "      BasicBlock-175            [-1, 189, 4, 4]               0\n",
            "     BatchNorm2d-176            [-1, 189, 4, 4]             378\n",
            "          Conv2d-177            [-1, 194, 4, 4]         329,994\n",
            "     BatchNorm2d-178            [-1, 194, 4, 4]             388\n",
            "            ReLU-179            [-1, 194, 4, 4]               0\n",
            "          Conv2d-180            [-1, 194, 4, 4]         338,724\n",
            "     BatchNorm2d-181            [-1, 194, 4, 4]             388\n",
            "      BasicBlock-182            [-1, 194, 4, 4]               0\n",
            "     BatchNorm2d-183            [-1, 194, 4, 4]             388\n",
            "          Conv2d-184            [-1, 199, 4, 4]         347,454\n",
            "     BatchNorm2d-185            [-1, 199, 4, 4]             398\n",
            "            ReLU-186            [-1, 199, 4, 4]               0\n",
            "          Conv2d-187            [-1, 199, 4, 4]         356,409\n",
            "     BatchNorm2d-188            [-1, 199, 4, 4]             398\n",
            "      BasicBlock-189            [-1, 199, 4, 4]               0\n",
            "     BatchNorm2d-190            [-1, 199, 4, 4]             398\n",
            "          Conv2d-191            [-1, 204, 2, 2]         365,364\n",
            "     BatchNorm2d-192            [-1, 204, 2, 2]             408\n",
            "            ReLU-193            [-1, 204, 2, 2]               0\n",
            "          Conv2d-194            [-1, 204, 2, 2]         374,544\n",
            "     BatchNorm2d-195            [-1, 204, 2, 2]             408\n",
            "         Dropout-196            [-1, 204, 2, 2]               0\n",
            "       AvgPool2d-197            [-1, 199, 2, 2]               0\n",
            "      BasicBlock-198            [-1, 204, 2, 2]               0\n",
            "     BatchNorm2d-199            [-1, 204, 2, 2]             408\n",
            "          Conv2d-200            [-1, 209, 2, 2]         383,724\n",
            "     BatchNorm2d-201            [-1, 209, 2, 2]             418\n",
            "            ReLU-202            [-1, 209, 2, 2]               0\n",
            "          Conv2d-203            [-1, 209, 2, 2]         393,129\n",
            "     BatchNorm2d-204            [-1, 209, 2, 2]             418\n",
            "      BasicBlock-205            [-1, 209, 2, 2]               0\n",
            "     BatchNorm2d-206            [-1, 209, 2, 2]             418\n",
            "          Conv2d-207            [-1, 215, 2, 2]         404,415\n",
            "     BatchNorm2d-208            [-1, 215, 2, 2]             430\n",
            "            ReLU-209            [-1, 215, 2, 2]               0\n",
            "          Conv2d-210            [-1, 215, 2, 2]         416,025\n",
            "     BatchNorm2d-211            [-1, 215, 2, 2]             430\n",
            "      BasicBlock-212            [-1, 215, 2, 2]               0\n",
            "     BatchNorm2d-213            [-1, 215, 2, 2]             430\n",
            "          Conv2d-214            [-1, 220, 2, 2]         425,700\n",
            "     BatchNorm2d-215            [-1, 220, 2, 2]             440\n",
            "            ReLU-216            [-1, 220, 2, 2]               0\n",
            "          Conv2d-217            [-1, 220, 2, 2]         435,600\n",
            "     BatchNorm2d-218            [-1, 220, 2, 2]             440\n",
            "      BasicBlock-219            [-1, 220, 2, 2]               0\n",
            "     BatchNorm2d-220            [-1, 220, 2, 2]             440\n",
            "          Conv2d-221            [-1, 225, 2, 2]         445,500\n",
            "     BatchNorm2d-222            [-1, 225, 2, 2]             450\n",
            "            ReLU-223            [-1, 225, 2, 2]               0\n",
            "          Conv2d-224            [-1, 225, 2, 2]         455,625\n",
            "     BatchNorm2d-225            [-1, 225, 2, 2]             450\n",
            "      BasicBlock-226            [-1, 225, 2, 2]               0\n",
            "     BatchNorm2d-227            [-1, 225, 2, 2]             450\n",
            "          Conv2d-228            [-1, 230, 2, 2]         465,750\n",
            "     BatchNorm2d-229            [-1, 230, 2, 2]             460\n",
            "            ReLU-230            [-1, 230, 2, 2]               0\n",
            "          Conv2d-231            [-1, 230, 2, 2]         476,100\n",
            "     BatchNorm2d-232            [-1, 230, 2, 2]             460\n",
            "      BasicBlock-233            [-1, 230, 2, 2]               0\n",
            "     BatchNorm2d-234            [-1, 230, 2, 2]             460\n",
            "          Conv2d-235            [-1, 235, 2, 2]         486,450\n",
            "     BatchNorm2d-236            [-1, 235, 2, 2]             470\n",
            "            ReLU-237            [-1, 235, 2, 2]               0\n",
            "          Conv2d-238            [-1, 235, 2, 2]         497,025\n",
            "     BatchNorm2d-239            [-1, 235, 2, 2]             470\n",
            "      BasicBlock-240            [-1, 235, 2, 2]               0\n",
            "     BatchNorm2d-241            [-1, 235, 2, 2]             470\n",
            "          Conv2d-242            [-1, 241, 2, 2]         509,715\n",
            "     BatchNorm2d-243            [-1, 241, 2, 2]             482\n",
            "            ReLU-244            [-1, 241, 2, 2]               0\n",
            "          Conv2d-245            [-1, 241, 2, 2]         522,729\n",
            "     BatchNorm2d-246            [-1, 241, 2, 2]             482\n",
            "      BasicBlock-247            [-1, 241, 2, 2]               0\n",
            "     BatchNorm2d-248            [-1, 241, 2, 2]             482\n",
            "          Conv2d-249            [-1, 246, 2, 2]         533,574\n",
            "     BatchNorm2d-250            [-1, 246, 2, 2]             492\n",
            "            ReLU-251            [-1, 246, 2, 2]               0\n",
            "          Conv2d-252            [-1, 246, 2, 2]         544,644\n",
            "     BatchNorm2d-253            [-1, 246, 2, 2]             492\n",
            "      BasicBlock-254            [-1, 246, 2, 2]               0\n",
            "     BatchNorm2d-255            [-1, 246, 2, 2]             492\n",
            "          Conv2d-256            [-1, 251, 2, 2]         555,714\n",
            "     BatchNorm2d-257            [-1, 251, 2, 2]             502\n",
            "            ReLU-258            [-1, 251, 2, 2]               0\n",
            "          Conv2d-259            [-1, 251, 2, 2]         567,009\n",
            "     BatchNorm2d-260            [-1, 251, 2, 2]             502\n",
            "      BasicBlock-261            [-1, 251, 2, 2]               0\n",
            "     BatchNorm2d-262            [-1, 251, 2, 2]             502\n",
            "          Conv2d-263            [-1, 256, 2, 2]         578,304\n",
            "     BatchNorm2d-264            [-1, 256, 2, 2]             512\n",
            "            ReLU-265            [-1, 256, 2, 2]               0\n",
            "          Conv2d-266            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-267            [-1, 256, 2, 2]             512\n",
            "      BasicBlock-268            [-1, 256, 2, 2]               0\n",
            "     BatchNorm2d-269            [-1, 256, 2, 2]             512\n",
            "          Conv2d-270            [-1, 261, 2, 2]         601,344\n",
            "     BatchNorm2d-271            [-1, 261, 2, 2]             522\n",
            "            ReLU-272            [-1, 261, 2, 2]               0\n",
            "          Conv2d-273            [-1, 261, 2, 2]         613,089\n",
            "     BatchNorm2d-274            [-1, 261, 2, 2]             522\n",
            "      BasicBlock-275            [-1, 261, 2, 2]               0\n",
            "     BatchNorm2d-276            [-1, 261, 2, 2]             522\n",
            "          Conv2d-277            [-1, 266, 2, 2]         624,834\n",
            "     BatchNorm2d-278            [-1, 266, 2, 2]             532\n",
            "            ReLU-279            [-1, 266, 2, 2]               0\n",
            "          Conv2d-280            [-1, 266, 2, 2]         636,804\n",
            "     BatchNorm2d-281            [-1, 266, 2, 2]             532\n",
            "      BasicBlock-282            [-1, 266, 2, 2]               0\n",
            "     BatchNorm2d-283            [-1, 266, 2, 2]             532\n",
            "          Conv2d-284            [-1, 272, 1, 1]         651,168\n",
            "     BatchNorm2d-285            [-1, 272, 1, 1]             544\n",
            "            ReLU-286            [-1, 272, 1, 1]               0\n",
            "          Conv2d-287            [-1, 272, 1, 1]         665,856\n",
            "     BatchNorm2d-288            [-1, 272, 1, 1]             544\n",
            "         Dropout-289            [-1, 272, 1, 1]               0\n",
            "       AvgPool2d-290            [-1, 266, 1, 1]               0\n",
            "      BasicBlock-291            [-1, 272, 1, 1]               0\n",
            "     BatchNorm2d-292            [-1, 272, 1, 1]             544\n",
            "          Conv2d-293            [-1, 277, 1, 1]         678,096\n",
            "     BatchNorm2d-294            [-1, 277, 1, 1]             554\n",
            "            ReLU-295            [-1, 277, 1, 1]               0\n",
            "          Conv2d-296            [-1, 277, 1, 1]         690,561\n",
            "     BatchNorm2d-297            [-1, 277, 1, 1]             554\n",
            "      BasicBlock-298            [-1, 277, 1, 1]               0\n",
            "     BatchNorm2d-299            [-1, 277, 1, 1]             554\n",
            "          Conv2d-300            [-1, 282, 1, 1]         703,026\n",
            "     BatchNorm2d-301            [-1, 282, 1, 1]             564\n",
            "            ReLU-302            [-1, 282, 1, 1]               0\n",
            "          Conv2d-303            [-1, 282, 1, 1]         715,716\n",
            "     BatchNorm2d-304            [-1, 282, 1, 1]             564\n",
            "      BasicBlock-305            [-1, 282, 1, 1]               0\n",
            "     BatchNorm2d-306            [-1, 282, 1, 1]             564\n",
            "          Conv2d-307            [-1, 287, 1, 1]         728,406\n",
            "     BatchNorm2d-308            [-1, 287, 1, 1]             574\n",
            "            ReLU-309            [-1, 287, 1, 1]               0\n",
            "          Conv2d-310            [-1, 287, 1, 1]         741,321\n",
            "     BatchNorm2d-311            [-1, 287, 1, 1]             574\n",
            "      BasicBlock-312            [-1, 287, 1, 1]               0\n",
            "     BatchNorm2d-313            [-1, 287, 1, 1]             574\n",
            "          Conv2d-314            [-1, 292, 1, 1]         754,236\n",
            "     BatchNorm2d-315            [-1, 292, 1, 1]             584\n",
            "            ReLU-316            [-1, 292, 1, 1]               0\n",
            "          Conv2d-317            [-1, 292, 1, 1]         767,376\n",
            "     BatchNorm2d-318            [-1, 292, 1, 1]             584\n",
            "      BasicBlock-319            [-1, 292, 1, 1]               0\n",
            "     BatchNorm2d-320            [-1, 292, 1, 1]             584\n",
            "          Conv2d-321            [-1, 298, 1, 1]         783,144\n",
            "     BatchNorm2d-322            [-1, 298, 1, 1]             596\n",
            "            ReLU-323            [-1, 298, 1, 1]               0\n",
            "          Conv2d-324            [-1, 298, 1, 1]         799,236\n",
            "     BatchNorm2d-325            [-1, 298, 1, 1]             596\n",
            "      BasicBlock-326            [-1, 298, 1, 1]               0\n",
            "     BatchNorm2d-327            [-1, 298, 1, 1]             596\n",
            "          Conv2d-328            [-1, 303, 1, 1]         812,646\n",
            "     BatchNorm2d-329            [-1, 303, 1, 1]             606\n",
            "            ReLU-330            [-1, 303, 1, 1]               0\n",
            "          Conv2d-331            [-1, 303, 1, 1]         826,281\n",
            "     BatchNorm2d-332            [-1, 303, 1, 1]             606\n",
            "      BasicBlock-333            [-1, 303, 1, 1]               0\n",
            "     BatchNorm2d-334            [-1, 303, 1, 1]             606\n",
            "          Conv2d-335            [-1, 308, 1, 1]         839,916\n",
            "     BatchNorm2d-336            [-1, 308, 1, 1]             616\n",
            "            ReLU-337            [-1, 308, 1, 1]               0\n",
            "          Conv2d-338            [-1, 308, 1, 1]         853,776\n",
            "     BatchNorm2d-339            [-1, 308, 1, 1]             616\n",
            "      BasicBlock-340            [-1, 308, 1, 1]               0\n",
            "     BatchNorm2d-341            [-1, 308, 1, 1]             616\n",
            "          Conv2d-342            [-1, 313, 1, 1]         867,636\n",
            "     BatchNorm2d-343            [-1, 313, 1, 1]             626\n",
            "            ReLU-344            [-1, 313, 1, 1]               0\n",
            "          Conv2d-345            [-1, 313, 1, 1]         881,721\n",
            "     BatchNorm2d-346            [-1, 313, 1, 1]             626\n",
            "      BasicBlock-347            [-1, 313, 1, 1]               0\n",
            "     BatchNorm2d-348            [-1, 313, 1, 1]             626\n",
            "          Conv2d-349            [-1, 318, 1, 1]         895,806\n",
            "     BatchNorm2d-350            [-1, 318, 1, 1]             636\n",
            "            ReLU-351            [-1, 318, 1, 1]               0\n",
            "          Conv2d-352            [-1, 318, 1, 1]         910,116\n",
            "     BatchNorm2d-353            [-1, 318, 1, 1]             636\n",
            "      BasicBlock-354            [-1, 318, 1, 1]               0\n",
            "     BatchNorm2d-355            [-1, 318, 1, 1]             636\n",
            "          Conv2d-356            [-1, 324, 1, 1]         927,288\n",
            "     BatchNorm2d-357            [-1, 324, 1, 1]             648\n",
            "            ReLU-358            [-1, 324, 1, 1]               0\n",
            "          Conv2d-359            [-1, 324, 1, 1]         944,784\n",
            "     BatchNorm2d-360            [-1, 324, 1, 1]             648\n",
            "      BasicBlock-361            [-1, 324, 1, 1]               0\n",
            "     BatchNorm2d-362            [-1, 324, 1, 1]             648\n",
            "          Conv2d-363            [-1, 329, 1, 1]         959,364\n",
            "     BatchNorm2d-364            [-1, 329, 1, 1]             658\n",
            "            ReLU-365            [-1, 329, 1, 1]               0\n",
            "          Conv2d-366            [-1, 329, 1, 1]         974,169\n",
            "     BatchNorm2d-367            [-1, 329, 1, 1]             658\n",
            "      BasicBlock-368            [-1, 329, 1, 1]               0\n",
            "     BatchNorm2d-369            [-1, 329, 1, 1]             658\n",
            "          Conv2d-370            [-1, 334, 1, 1]         988,974\n",
            "     BatchNorm2d-371            [-1, 334, 1, 1]             668\n",
            "            ReLU-372            [-1, 334, 1, 1]               0\n",
            "          Conv2d-373            [-1, 334, 1, 1]       1,004,004\n",
            "     BatchNorm2d-374            [-1, 334, 1, 1]             668\n",
            "      BasicBlock-375            [-1, 334, 1, 1]               0\n",
            "     BatchNorm2d-376            [-1, 334, 1, 1]             668\n",
            "            ReLU-377            [-1, 334, 1, 1]               0\n",
            "          Linear-378                   [-1, 10]           3,350\n",
            "================================================================\n",
            "Total params: 43,307,727\n",
            "Trainable params: 43,307,727\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.71\n",
            "Params size (MB): 165.21\n",
            "Estimated Total Size (MB): 172.93\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "=> the layer configuration for each stage is set to [13, 13, 13, 13]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
            "            Conv2d-6             [-1, 69, 8, 8]          39,744\n",
            "       BatchNorm2d-7             [-1, 69, 8, 8]             138\n",
            "              ReLU-8             [-1, 69, 8, 8]               0\n",
            "            Conv2d-9             [-1, 69, 8, 8]          42,849\n",
            "      BatchNorm2d-10             [-1, 69, 8, 8]             138\n",
            "       BasicBlock-11             [-1, 69, 8, 8]               0\n",
            "      BatchNorm2d-12             [-1, 69, 8, 8]             138\n",
            "           Conv2d-13             [-1, 74, 8, 8]          45,954\n",
            "      BatchNorm2d-14             [-1, 74, 8, 8]             148\n",
            "             ReLU-15             [-1, 74, 8, 8]               0\n",
            "           Conv2d-16             [-1, 74, 8, 8]          49,284\n",
            "      BatchNorm2d-17             [-1, 74, 8, 8]             148\n",
            "       BasicBlock-18             [-1, 74, 8, 8]               0\n",
            "      BatchNorm2d-19             [-1, 74, 8, 8]             148\n",
            "           Conv2d-20             [-1, 80, 8, 8]          53,280\n",
            "      BatchNorm2d-21             [-1, 80, 8, 8]             160\n",
            "             ReLU-22             [-1, 80, 8, 8]               0\n",
            "           Conv2d-23             [-1, 80, 8, 8]          57,600\n",
            "      BatchNorm2d-24             [-1, 80, 8, 8]             160\n",
            "       BasicBlock-25             [-1, 80, 8, 8]               0\n",
            "      BatchNorm2d-26             [-1, 80, 8, 8]             160\n",
            "           Conv2d-27             [-1, 85, 8, 8]          61,200\n",
            "      BatchNorm2d-28             [-1, 85, 8, 8]             170\n",
            "             ReLU-29             [-1, 85, 8, 8]               0\n",
            "           Conv2d-30             [-1, 85, 8, 8]          65,025\n",
            "      BatchNorm2d-31             [-1, 85, 8, 8]             170\n",
            "       BasicBlock-32             [-1, 85, 8, 8]               0\n",
            "      BatchNorm2d-33             [-1, 85, 8, 8]             170\n",
            "           Conv2d-34             [-1, 90, 8, 8]          68,850\n",
            "      BatchNorm2d-35             [-1, 90, 8, 8]             180\n",
            "             ReLU-36             [-1, 90, 8, 8]               0\n",
            "           Conv2d-37             [-1, 90, 8, 8]          72,900\n",
            "      BatchNorm2d-38             [-1, 90, 8, 8]             180\n",
            "       BasicBlock-39             [-1, 90, 8, 8]               0\n",
            "      BatchNorm2d-40             [-1, 90, 8, 8]             180\n",
            "           Conv2d-41             [-1, 95, 8, 8]          76,950\n",
            "      BatchNorm2d-42             [-1, 95, 8, 8]             190\n",
            "             ReLU-43             [-1, 95, 8, 8]               0\n",
            "           Conv2d-44             [-1, 95, 8, 8]          81,225\n",
            "      BatchNorm2d-45             [-1, 95, 8, 8]             190\n",
            "       BasicBlock-46             [-1, 95, 8, 8]               0\n",
            "      BatchNorm2d-47             [-1, 95, 8, 8]             190\n",
            "           Conv2d-48            [-1, 100, 8, 8]          85,500\n",
            "      BatchNorm2d-49            [-1, 100, 8, 8]             200\n",
            "             ReLU-50            [-1, 100, 8, 8]               0\n",
            "           Conv2d-51            [-1, 100, 8, 8]          90,000\n",
            "      BatchNorm2d-52            [-1, 100, 8, 8]             200\n",
            "       BasicBlock-53            [-1, 100, 8, 8]               0\n",
            "      BatchNorm2d-54            [-1, 100, 8, 8]             200\n",
            "           Conv2d-55            [-1, 106, 8, 8]          95,400\n",
            "      BatchNorm2d-56            [-1, 106, 8, 8]             212\n",
            "             ReLU-57            [-1, 106, 8, 8]               0\n",
            "           Conv2d-58            [-1, 106, 8, 8]         101,124\n",
            "      BatchNorm2d-59            [-1, 106, 8, 8]             212\n",
            "       BasicBlock-60            [-1, 106, 8, 8]               0\n",
            "      BatchNorm2d-61            [-1, 106, 8, 8]             212\n",
            "           Conv2d-62            [-1, 111, 8, 8]         105,894\n",
            "      BatchNorm2d-63            [-1, 111, 8, 8]             222\n",
            "             ReLU-64            [-1, 111, 8, 8]               0\n",
            "           Conv2d-65            [-1, 111, 8, 8]         110,889\n",
            "      BatchNorm2d-66            [-1, 111, 8, 8]             222\n",
            "       BasicBlock-67            [-1, 111, 8, 8]               0\n",
            "      BatchNorm2d-68            [-1, 111, 8, 8]             222\n",
            "           Conv2d-69            [-1, 116, 8, 8]         115,884\n",
            "      BatchNorm2d-70            [-1, 116, 8, 8]             232\n",
            "             ReLU-71            [-1, 116, 8, 8]               0\n",
            "           Conv2d-72            [-1, 116, 8, 8]         121,104\n",
            "      BatchNorm2d-73            [-1, 116, 8, 8]             232\n",
            "       BasicBlock-74            [-1, 116, 8, 8]               0\n",
            "      BatchNorm2d-75            [-1, 116, 8, 8]             232\n",
            "           Conv2d-76            [-1, 121, 8, 8]         126,324\n",
            "      BatchNorm2d-77            [-1, 121, 8, 8]             242\n",
            "             ReLU-78            [-1, 121, 8, 8]               0\n",
            "           Conv2d-79            [-1, 121, 8, 8]         131,769\n",
            "      BatchNorm2d-80            [-1, 121, 8, 8]             242\n",
            "       BasicBlock-81            [-1, 121, 8, 8]               0\n",
            "      BatchNorm2d-82            [-1, 121, 8, 8]             242\n",
            "           Conv2d-83            [-1, 126, 8, 8]         137,214\n",
            "      BatchNorm2d-84            [-1, 126, 8, 8]             252\n",
            "             ReLU-85            [-1, 126, 8, 8]               0\n",
            "           Conv2d-86            [-1, 126, 8, 8]         142,884\n",
            "      BatchNorm2d-87            [-1, 126, 8, 8]             252\n",
            "       BasicBlock-88            [-1, 126, 8, 8]               0\n",
            "      BatchNorm2d-89            [-1, 126, 8, 8]             252\n",
            "           Conv2d-90            [-1, 132, 8, 8]         149,688\n",
            "      BatchNorm2d-91            [-1, 132, 8, 8]             264\n",
            "             ReLU-92            [-1, 132, 8, 8]               0\n",
            "           Conv2d-93            [-1, 132, 8, 8]         156,816\n",
            "      BatchNorm2d-94            [-1, 132, 8, 8]             264\n",
            "       BasicBlock-95            [-1, 132, 8, 8]               0\n",
            "      BatchNorm2d-96            [-1, 132, 8, 8]             264\n",
            "           Conv2d-97            [-1, 137, 4, 4]         162,756\n",
            "      BatchNorm2d-98            [-1, 137, 4, 4]             274\n",
            "             ReLU-99            [-1, 137, 4, 4]               0\n",
            "          Conv2d-100            [-1, 137, 4, 4]         168,921\n",
            "     BatchNorm2d-101            [-1, 137, 4, 4]             274\n",
            "       AvgPool2d-102            [-1, 132, 4, 4]               0\n",
            "      BasicBlock-103            [-1, 137, 4, 4]               0\n",
            "     BatchNorm2d-104            [-1, 137, 4, 4]             274\n",
            "          Conv2d-105            [-1, 142, 4, 4]         175,086\n",
            "     BatchNorm2d-106            [-1, 142, 4, 4]             284\n",
            "            ReLU-107            [-1, 142, 4, 4]               0\n",
            "          Conv2d-108            [-1, 142, 4, 4]         181,476\n",
            "     BatchNorm2d-109            [-1, 142, 4, 4]             284\n",
            "      BasicBlock-110            [-1, 142, 4, 4]               0\n",
            "     BatchNorm2d-111            [-1, 142, 4, 4]             284\n",
            "          Conv2d-112            [-1, 147, 4, 4]         187,866\n",
            "     BatchNorm2d-113            [-1, 147, 4, 4]             294\n",
            "            ReLU-114            [-1, 147, 4, 4]               0\n",
            "          Conv2d-115            [-1, 147, 4, 4]         194,481\n",
            "     BatchNorm2d-116            [-1, 147, 4, 4]             294\n",
            "      BasicBlock-117            [-1, 147, 4, 4]               0\n",
            "     BatchNorm2d-118            [-1, 147, 4, 4]             294\n",
            "          Conv2d-119            [-1, 152, 4, 4]         201,096\n",
            "     BatchNorm2d-120            [-1, 152, 4, 4]             304\n",
            "            ReLU-121            [-1, 152, 4, 4]               0\n",
            "          Conv2d-122            [-1, 152, 4, 4]         207,936\n",
            "     BatchNorm2d-123            [-1, 152, 4, 4]             304\n",
            "      BasicBlock-124            [-1, 152, 4, 4]               0\n",
            "     BatchNorm2d-125            [-1, 152, 4, 4]             304\n",
            "          Conv2d-126            [-1, 157, 4, 4]         214,776\n",
            "     BatchNorm2d-127            [-1, 157, 4, 4]             314\n",
            "            ReLU-128            [-1, 157, 4, 4]               0\n",
            "          Conv2d-129            [-1, 157, 4, 4]         221,841\n",
            "     BatchNorm2d-130            [-1, 157, 4, 4]             314\n",
            "      BasicBlock-131            [-1, 157, 4, 4]               0\n",
            "     BatchNorm2d-132            [-1, 157, 4, 4]             314\n",
            "          Conv2d-133            [-1, 163, 4, 4]         230,319\n",
            "     BatchNorm2d-134            [-1, 163, 4, 4]             326\n",
            "            ReLU-135            [-1, 163, 4, 4]               0\n",
            "          Conv2d-136            [-1, 163, 4, 4]         239,121\n",
            "     BatchNorm2d-137            [-1, 163, 4, 4]             326\n",
            "      BasicBlock-138            [-1, 163, 4, 4]               0\n",
            "     BatchNorm2d-139            [-1, 163, 4, 4]             326\n",
            "          Conv2d-140            [-1, 168, 4, 4]         246,456\n",
            "     BatchNorm2d-141            [-1, 168, 4, 4]             336\n",
            "            ReLU-142            [-1, 168, 4, 4]               0\n",
            "          Conv2d-143            [-1, 168, 4, 4]         254,016\n",
            "     BatchNorm2d-144            [-1, 168, 4, 4]             336\n",
            "      BasicBlock-145            [-1, 168, 4, 4]               0\n",
            "     BatchNorm2d-146            [-1, 168, 4, 4]             336\n",
            "          Conv2d-147            [-1, 173, 4, 4]         261,576\n",
            "     BatchNorm2d-148            [-1, 173, 4, 4]             346\n",
            "            ReLU-149            [-1, 173, 4, 4]               0\n",
            "          Conv2d-150            [-1, 173, 4, 4]         269,361\n",
            "     BatchNorm2d-151            [-1, 173, 4, 4]             346\n",
            "      BasicBlock-152            [-1, 173, 4, 4]               0\n",
            "     BatchNorm2d-153            [-1, 173, 4, 4]             346\n",
            "          Conv2d-154            [-1, 178, 4, 4]         277,146\n",
            "     BatchNorm2d-155            [-1, 178, 4, 4]             356\n",
            "            ReLU-156            [-1, 178, 4, 4]               0\n",
            "          Conv2d-157            [-1, 178, 4, 4]         285,156\n",
            "     BatchNorm2d-158            [-1, 178, 4, 4]             356\n",
            "      BasicBlock-159            [-1, 178, 4, 4]               0\n",
            "     BatchNorm2d-160            [-1, 178, 4, 4]             356\n",
            "          Conv2d-161            [-1, 183, 4, 4]         293,166\n",
            "     BatchNorm2d-162            [-1, 183, 4, 4]             366\n",
            "            ReLU-163            [-1, 183, 4, 4]               0\n",
            "          Conv2d-164            [-1, 183, 4, 4]         301,401\n",
            "     BatchNorm2d-165            [-1, 183, 4, 4]             366\n",
            "      BasicBlock-166            [-1, 183, 4, 4]               0\n",
            "     BatchNorm2d-167            [-1, 183, 4, 4]             366\n",
            "          Conv2d-168            [-1, 189, 4, 4]         311,283\n",
            "     BatchNorm2d-169            [-1, 189, 4, 4]             378\n",
            "            ReLU-170            [-1, 189, 4, 4]               0\n",
            "          Conv2d-171            [-1, 189, 4, 4]         321,489\n",
            "     BatchNorm2d-172            [-1, 189, 4, 4]             378\n",
            "      BasicBlock-173            [-1, 189, 4, 4]               0\n",
            "     BatchNorm2d-174            [-1, 189, 4, 4]             378\n",
            "          Conv2d-175            [-1, 194, 4, 4]         329,994\n",
            "     BatchNorm2d-176            [-1, 194, 4, 4]             388\n",
            "            ReLU-177            [-1, 194, 4, 4]               0\n",
            "          Conv2d-178            [-1, 194, 4, 4]         338,724\n",
            "     BatchNorm2d-179            [-1, 194, 4, 4]             388\n",
            "      BasicBlock-180            [-1, 194, 4, 4]               0\n",
            "     BatchNorm2d-181            [-1, 194, 4, 4]             388\n",
            "          Conv2d-182            [-1, 199, 4, 4]         347,454\n",
            "     BatchNorm2d-183            [-1, 199, 4, 4]             398\n",
            "            ReLU-184            [-1, 199, 4, 4]               0\n",
            "          Conv2d-185            [-1, 199, 4, 4]         356,409\n",
            "     BatchNorm2d-186            [-1, 199, 4, 4]             398\n",
            "      BasicBlock-187            [-1, 199, 4, 4]               0\n",
            "     BatchNorm2d-188            [-1, 199, 4, 4]             398\n",
            "          Conv2d-189            [-1, 204, 2, 2]         365,364\n",
            "     BatchNorm2d-190            [-1, 204, 2, 2]             408\n",
            "            ReLU-191            [-1, 204, 2, 2]               0\n",
            "          Conv2d-192            [-1, 204, 2, 2]         374,544\n",
            "     BatchNorm2d-193            [-1, 204, 2, 2]             408\n",
            "       AvgPool2d-194            [-1, 199, 2, 2]               0\n",
            "      BasicBlock-195            [-1, 204, 2, 2]               0\n",
            "     BatchNorm2d-196            [-1, 204, 2, 2]             408\n",
            "          Conv2d-197            [-1, 209, 2, 2]         383,724\n",
            "     BatchNorm2d-198            [-1, 209, 2, 2]             418\n",
            "            ReLU-199            [-1, 209, 2, 2]               0\n",
            "          Conv2d-200            [-1, 209, 2, 2]         393,129\n",
            "     BatchNorm2d-201            [-1, 209, 2, 2]             418\n",
            "      BasicBlock-202            [-1, 209, 2, 2]               0\n",
            "     BatchNorm2d-203            [-1, 209, 2, 2]             418\n",
            "          Conv2d-204            [-1, 215, 2, 2]         404,415\n",
            "     BatchNorm2d-205            [-1, 215, 2, 2]             430\n",
            "            ReLU-206            [-1, 215, 2, 2]               0\n",
            "          Conv2d-207            [-1, 215, 2, 2]         416,025\n",
            "     BatchNorm2d-208            [-1, 215, 2, 2]             430\n",
            "      BasicBlock-209            [-1, 215, 2, 2]               0\n",
            "     BatchNorm2d-210            [-1, 215, 2, 2]             430\n",
            "          Conv2d-211            [-1, 220, 2, 2]         425,700\n",
            "     BatchNorm2d-212            [-1, 220, 2, 2]             440\n",
            "            ReLU-213            [-1, 220, 2, 2]               0\n",
            "          Conv2d-214            [-1, 220, 2, 2]         435,600\n",
            "     BatchNorm2d-215            [-1, 220, 2, 2]             440\n",
            "      BasicBlock-216            [-1, 220, 2, 2]               0\n",
            "     BatchNorm2d-217            [-1, 220, 2, 2]             440\n",
            "          Conv2d-218            [-1, 225, 2, 2]         445,500\n",
            "     BatchNorm2d-219            [-1, 225, 2, 2]             450\n",
            "            ReLU-220            [-1, 225, 2, 2]               0\n",
            "          Conv2d-221            [-1, 225, 2, 2]         455,625\n",
            "     BatchNorm2d-222            [-1, 225, 2, 2]             450\n",
            "      BasicBlock-223            [-1, 225, 2, 2]               0\n",
            "     BatchNorm2d-224            [-1, 225, 2, 2]             450\n",
            "          Conv2d-225            [-1, 230, 2, 2]         465,750\n",
            "     BatchNorm2d-226            [-1, 230, 2, 2]             460\n",
            "            ReLU-227            [-1, 230, 2, 2]               0\n",
            "          Conv2d-228            [-1, 230, 2, 2]         476,100\n",
            "     BatchNorm2d-229            [-1, 230, 2, 2]             460\n",
            "      BasicBlock-230            [-1, 230, 2, 2]               0\n",
            "     BatchNorm2d-231            [-1, 230, 2, 2]             460\n",
            "          Conv2d-232            [-1, 235, 2, 2]         486,450\n",
            "     BatchNorm2d-233            [-1, 235, 2, 2]             470\n",
            "            ReLU-234            [-1, 235, 2, 2]               0\n",
            "          Conv2d-235            [-1, 235, 2, 2]         497,025\n",
            "     BatchNorm2d-236            [-1, 235, 2, 2]             470\n",
            "      BasicBlock-237            [-1, 235, 2, 2]               0\n",
            "     BatchNorm2d-238            [-1, 235, 2, 2]             470\n",
            "          Conv2d-239            [-1, 241, 2, 2]         509,715\n",
            "     BatchNorm2d-240            [-1, 241, 2, 2]             482\n",
            "            ReLU-241            [-1, 241, 2, 2]               0\n",
            "          Conv2d-242            [-1, 241, 2, 2]         522,729\n",
            "     BatchNorm2d-243            [-1, 241, 2, 2]             482\n",
            "      BasicBlock-244            [-1, 241, 2, 2]               0\n",
            "     BatchNorm2d-245            [-1, 241, 2, 2]             482\n",
            "          Conv2d-246            [-1, 246, 2, 2]         533,574\n",
            "     BatchNorm2d-247            [-1, 246, 2, 2]             492\n",
            "            ReLU-248            [-1, 246, 2, 2]               0\n",
            "          Conv2d-249            [-1, 246, 2, 2]         544,644\n",
            "     BatchNorm2d-250            [-1, 246, 2, 2]             492\n",
            "      BasicBlock-251            [-1, 246, 2, 2]               0\n",
            "     BatchNorm2d-252            [-1, 246, 2, 2]             492\n",
            "          Conv2d-253            [-1, 251, 2, 2]         555,714\n",
            "     BatchNorm2d-254            [-1, 251, 2, 2]             502\n",
            "            ReLU-255            [-1, 251, 2, 2]               0\n",
            "          Conv2d-256            [-1, 251, 2, 2]         567,009\n",
            "     BatchNorm2d-257            [-1, 251, 2, 2]             502\n",
            "      BasicBlock-258            [-1, 251, 2, 2]               0\n",
            "     BatchNorm2d-259            [-1, 251, 2, 2]             502\n",
            "          Conv2d-260            [-1, 256, 2, 2]         578,304\n",
            "     BatchNorm2d-261            [-1, 256, 2, 2]             512\n",
            "            ReLU-262            [-1, 256, 2, 2]               0\n",
            "          Conv2d-263            [-1, 256, 2, 2]         589,824\n",
            "     BatchNorm2d-264            [-1, 256, 2, 2]             512\n",
            "      BasicBlock-265            [-1, 256, 2, 2]               0\n",
            "     BatchNorm2d-266            [-1, 256, 2, 2]             512\n",
            "          Conv2d-267            [-1, 261, 2, 2]         601,344\n",
            "     BatchNorm2d-268            [-1, 261, 2, 2]             522\n",
            "            ReLU-269            [-1, 261, 2, 2]               0\n",
            "          Conv2d-270            [-1, 261, 2, 2]         613,089\n",
            "     BatchNorm2d-271            [-1, 261, 2, 2]             522\n",
            "      BasicBlock-272            [-1, 261, 2, 2]               0\n",
            "     BatchNorm2d-273            [-1, 261, 2, 2]             522\n",
            "          Conv2d-274            [-1, 266, 2, 2]         624,834\n",
            "     BatchNorm2d-275            [-1, 266, 2, 2]             532\n",
            "            ReLU-276            [-1, 266, 2, 2]               0\n",
            "          Conv2d-277            [-1, 266, 2, 2]         636,804\n",
            "     BatchNorm2d-278            [-1, 266, 2, 2]             532\n",
            "      BasicBlock-279            [-1, 266, 2, 2]               0\n",
            "     BatchNorm2d-280            [-1, 266, 2, 2]             532\n",
            "          Conv2d-281            [-1, 272, 1, 1]         651,168\n",
            "     BatchNorm2d-282            [-1, 272, 1, 1]             544\n",
            "            ReLU-283            [-1, 272, 1, 1]               0\n",
            "          Conv2d-284            [-1, 272, 1, 1]         665,856\n",
            "     BatchNorm2d-285            [-1, 272, 1, 1]             544\n",
            "       AvgPool2d-286            [-1, 266, 1, 1]               0\n",
            "      BasicBlock-287            [-1, 272, 1, 1]               0\n",
            "     BatchNorm2d-288            [-1, 272, 1, 1]             544\n",
            "          Conv2d-289            [-1, 277, 1, 1]         678,096\n",
            "     BatchNorm2d-290            [-1, 277, 1, 1]             554\n",
            "            ReLU-291            [-1, 277, 1, 1]               0\n",
            "          Conv2d-292            [-1, 277, 1, 1]         690,561\n",
            "     BatchNorm2d-293            [-1, 277, 1, 1]             554\n",
            "      BasicBlock-294            [-1, 277, 1, 1]               0\n",
            "     BatchNorm2d-295            [-1, 277, 1, 1]             554\n",
            "          Conv2d-296            [-1, 282, 1, 1]         703,026\n",
            "     BatchNorm2d-297            [-1, 282, 1, 1]             564\n",
            "            ReLU-298            [-1, 282, 1, 1]               0\n",
            "          Conv2d-299            [-1, 282, 1, 1]         715,716\n",
            "     BatchNorm2d-300            [-1, 282, 1, 1]             564\n",
            "      BasicBlock-301            [-1, 282, 1, 1]               0\n",
            "     BatchNorm2d-302            [-1, 282, 1, 1]             564\n",
            "          Conv2d-303            [-1, 287, 1, 1]         728,406\n",
            "     BatchNorm2d-304            [-1, 287, 1, 1]             574\n",
            "            ReLU-305            [-1, 287, 1, 1]               0\n",
            "          Conv2d-306            [-1, 287, 1, 1]         741,321\n",
            "     BatchNorm2d-307            [-1, 287, 1, 1]             574\n",
            "      BasicBlock-308            [-1, 287, 1, 1]               0\n",
            "     BatchNorm2d-309            [-1, 287, 1, 1]             574\n",
            "          Conv2d-310            [-1, 292, 1, 1]         754,236\n",
            "     BatchNorm2d-311            [-1, 292, 1, 1]             584\n",
            "            ReLU-312            [-1, 292, 1, 1]               0\n",
            "          Conv2d-313            [-1, 292, 1, 1]         767,376\n",
            "     BatchNorm2d-314            [-1, 292, 1, 1]             584\n",
            "      BasicBlock-315            [-1, 292, 1, 1]               0\n",
            "     BatchNorm2d-316            [-1, 292, 1, 1]             584\n",
            "          Conv2d-317            [-1, 298, 1, 1]         783,144\n",
            "     BatchNorm2d-318            [-1, 298, 1, 1]             596\n",
            "            ReLU-319            [-1, 298, 1, 1]               0\n",
            "          Conv2d-320            [-1, 298, 1, 1]         799,236\n",
            "     BatchNorm2d-321            [-1, 298, 1, 1]             596\n",
            "      BasicBlock-322            [-1, 298, 1, 1]               0\n",
            "     BatchNorm2d-323            [-1, 298, 1, 1]             596\n",
            "          Conv2d-324            [-1, 303, 1, 1]         812,646\n",
            "     BatchNorm2d-325            [-1, 303, 1, 1]             606\n",
            "            ReLU-326            [-1, 303, 1, 1]               0\n",
            "          Conv2d-327            [-1, 303, 1, 1]         826,281\n",
            "     BatchNorm2d-328            [-1, 303, 1, 1]             606\n",
            "      BasicBlock-329            [-1, 303, 1, 1]               0\n",
            "     BatchNorm2d-330            [-1, 303, 1, 1]             606\n",
            "          Conv2d-331            [-1, 308, 1, 1]         839,916\n",
            "     BatchNorm2d-332            [-1, 308, 1, 1]             616\n",
            "            ReLU-333            [-1, 308, 1, 1]               0\n",
            "          Conv2d-334            [-1, 308, 1, 1]         853,776\n",
            "     BatchNorm2d-335            [-1, 308, 1, 1]             616\n",
            "      BasicBlock-336            [-1, 308, 1, 1]               0\n",
            "     BatchNorm2d-337            [-1, 308, 1, 1]             616\n",
            "          Conv2d-338            [-1, 313, 1, 1]         867,636\n",
            "     BatchNorm2d-339            [-1, 313, 1, 1]             626\n",
            "            ReLU-340            [-1, 313, 1, 1]               0\n",
            "          Conv2d-341            [-1, 313, 1, 1]         881,721\n",
            "     BatchNorm2d-342            [-1, 313, 1, 1]             626\n",
            "      BasicBlock-343            [-1, 313, 1, 1]               0\n",
            "     BatchNorm2d-344            [-1, 313, 1, 1]             626\n",
            "          Conv2d-345            [-1, 318, 1, 1]         895,806\n",
            "     BatchNorm2d-346            [-1, 318, 1, 1]             636\n",
            "            ReLU-347            [-1, 318, 1, 1]               0\n",
            "          Conv2d-348            [-1, 318, 1, 1]         910,116\n",
            "     BatchNorm2d-349            [-1, 318, 1, 1]             636\n",
            "      BasicBlock-350            [-1, 318, 1, 1]               0\n",
            "     BatchNorm2d-351            [-1, 318, 1, 1]             636\n",
            "          Conv2d-352            [-1, 324, 1, 1]         927,288\n",
            "     BatchNorm2d-353            [-1, 324, 1, 1]             648\n",
            "            ReLU-354            [-1, 324, 1, 1]               0\n",
            "          Conv2d-355            [-1, 324, 1, 1]         944,784\n",
            "     BatchNorm2d-356            [-1, 324, 1, 1]             648\n",
            "      BasicBlock-357            [-1, 324, 1, 1]               0\n",
            "     BatchNorm2d-358            [-1, 324, 1, 1]             648\n",
            "          Conv2d-359            [-1, 329, 1, 1]         959,364\n",
            "     BatchNorm2d-360            [-1, 329, 1, 1]             658\n",
            "            ReLU-361            [-1, 329, 1, 1]               0\n",
            "          Conv2d-362            [-1, 329, 1, 1]         974,169\n",
            "     BatchNorm2d-363            [-1, 329, 1, 1]             658\n",
            "      BasicBlock-364            [-1, 329, 1, 1]               0\n",
            "     BatchNorm2d-365            [-1, 329, 1, 1]             658\n",
            "          Conv2d-366            [-1, 334, 1, 1]         988,974\n",
            "     BatchNorm2d-367            [-1, 334, 1, 1]             668\n",
            "            ReLU-368            [-1, 334, 1, 1]               0\n",
            "          Conv2d-369            [-1, 334, 1, 1]       1,004,004\n",
            "     BatchNorm2d-370            [-1, 334, 1, 1]             668\n",
            "      BasicBlock-371            [-1, 334, 1, 1]               0\n",
            "     BatchNorm2d-372            [-1, 334, 1, 1]             668\n",
            "            ReLU-373            [-1, 334, 1, 1]               0\n",
            "          Linear-374                   [-1, 10]           3,350\n",
            "================================================================\n",
            "Total params: 43,307,727\n",
            "Trainable params: 43,307,727\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.65\n",
            "Params size (MB): 165.21\n",
            "Estimated Total Size (MB): 172.87\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "HNT7QPmNfd2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we write the code to train our network "
      ],
      "metadata": {
        "id": "ekXuEGamfe87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = OrderedDict(\n",
        "    lr = [.001],\n",
        "    batch_size = [64],\n",
        "    shuffle = [True],\n",
        "    arch = [\"pyramidNet-110\"],\n",
        "    dataset = [\"Cifar10\"],\n",
        "    act_func = [\"ReLU\"],\n",
        "    opt = [\"ADAM\"],\n",
        "    reg = ['shake-drop']\n",
        "\n",
        ")\n",
        "epochs = 30\n",
        "# Number of batches to log from the test data for each test step\n",
        "# (default set low to simplify demo)\n",
        "NUM_BATCHES_TO_LOG = 10 #79\n",
        "\n",
        "# Number of images to log per test batch\n",
        "# (default set low to simplify demo)\n",
        "NUM_IMAGES_PER_BATCH = 32 #128"
      ],
      "metadata": {
        "id": "c9FQtRiQ1bfW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu0NpJOz1bfW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61707ad8-de7b-441d-ba90-d832e2557cc3"
      },
      "source": [
        "m = RunManager()\n",
        "\n",
        "\n",
        "\n",
        "for run in RunBuilder.get_runs(params):\n",
        "    \n",
        "    model = PyramidNet_ShakeDrop(depth=110, alpha=270, num_classes=10, reg=run.reg).to(device)\n",
        "    \n",
        "    if run.act_func == \"Tanh\":\n",
        "        model.relu = nn.Tanh()\n",
        "    \n",
        "    if run.opt == \"ADAM\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=run.lr)\n",
        "    elif run.opt == \"SGD\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=run.lr)\n",
        "    \n",
        "    m.begin_run(run, model, train_loader,val_loader)\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        m.begin_epoch()\n",
        "        batch_count = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            images = batch[0]\n",
        "            labels = batch[1]\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "            loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            m.track_loss(loss)\n",
        "            m.track_num_correct_train(preds, labels)\n",
        "        for batch in tqdm(val_loader):\n",
        "        \n",
        "            images = batch[0]\n",
        "            labels = batch[1]\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "            loss = F.cross_entropy(preds, labels)\n",
        "            m.track_val_loss(loss)\n",
        "            m.track_num_correct_test(preds, labels)\n",
        "        \n",
        "        m.end_epoch()\n",
        "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
        "    for digit in range(10):\n",
        "      columns.append(\"score_\" + str(digit))\n",
        "    test_table = wandb.Table(columns=columns)\n",
        "    log_counter = 0\n",
        "    sum = 0\n",
        "    for batch in test_loader:\n",
        "            \n",
        "        images = batch[0]\n",
        "        labels = batch[1]\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        preds = model(images)\n",
        "        _, predicted = torch.max(preds.data, 1)\n",
        "        #loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "        #optimizer.zero_grad()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        if log_counter < NUM_BATCHES_TO_LOG:\n",
        "            log_test_predictions(images, labels, preds, predicted, test_table, log_counter)\n",
        "            log_counter += 1\n",
        "        #m.track_val_loss(loss)\n",
        "        sum += get_num_correct(preds, labels)\n",
        "    print('Test Accuracy is ',sum/len(test_loader.dataset))\n",
        "    wandb.log({\"test_predictions\" : test_table})\n",
        "    m.end_run()\n",
        "\n",
        "# when all runs are done, save results to files\n",
        "m.save('results_pyramidNet-110')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> the layer configuration for each stage is set to [13, 13, 13, 13]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230510_010304-oscoffk5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dl-codes/dl_project_2023/runs/oscoffk5' target=\"_blank\">light-blaze-20</a></strong> to <a href='https://wandb.ai/dl-codes/dl_project_2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dl-codes/dl_project_2023' target=\"_blank\">https://wandb.ai/dl-codes/dl_project_2023</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dl-codes/dl_project_2023/runs/oscoffk5' target=\"_blank\">https://wandb.ai/dl-codes/dl_project_2023/runs/oscoffk5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:21<00:00,  7.70it/s]\n",
            "100%|| 157/157 [00:10<00:00, 14.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.433625 train_loss =  1.5365392286300659 Validation_acc =  0.521 Validation_loss =  1.3408471130371094\n",
            "Training Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.56675 train_loss =  1.2164728908538818 Validation_acc =  0.5787 Validation_loss =  1.1978695205688477\n",
            "Training Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.79it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.624675 train_loss =  1.058074941921234 Validation_acc =  0.6153 Validation_loss =  1.0904589908599853\n",
            "Training Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.653725 train_loss =  0.9838202583312988 Validation_acc =  0.6519 Validation_loss =  1.0084903568267822\n",
            "Training Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:09<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.68995 train_loss =  0.8896305577278137 Validation_acc =  0.6822 Validation_loss =  0.9233267276763916\n",
            "Training Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 16.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.70935 train_loss =  0.8341146169662476 Validation_acc =  0.687 Validation_loss =  0.8996012660980225\n",
            "Training Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 17.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7253 train_loss =  0.7855721385478973 Validation_acc =  0.71 Validation_loss =  0.8418214307785035\n",
            "Training Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.72it/s]\n",
            "100%|| 157/157 [00:09<00:00, 16.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.74275 train_loss =  0.7410576807022095 Validation_acc =  0.7223 Validation_loss =  0.8124038204193115\n",
            "Training Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.757375 train_loss =  0.7035673981189727 Validation_acc =  0.7388 Validation_loss =  0.7616951150894165\n",
            "Training Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.764425 train_loss =  0.6743925714492798 Validation_acc =  0.743 Validation_loss =  0.7479339912414551\n",
            "Training Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.773975 train_loss =  0.6450283217430115 Validation_acc =  0.7525 Validation_loss =  0.7364486087799073\n",
            "Training Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:23<00:00,  7.53it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.788575 train_loss =  0.6115480856418609 Validation_acc =  0.7547 Validation_loss =  0.7305671298980713\n",
            "Training Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.76it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7956 train_loss =  0.5902421311378478 Validation_acc =  0.7645 Validation_loss =  0.698031914138794\n",
            "Training Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.75it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.8014 train_loss =  0.5702185067176819 Validation_acc =  0.752 Validation_loss =  0.719606861782074\n",
            "Training Epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.80985 train_loss =  0.5422398887395858 Validation_acc =  0.7682 Validation_loss =  0.68189271068573\n",
            "Training Epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:09<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.817125 train_loss =  0.5244234524011612 Validation_acc =  0.7798 Validation_loss =  0.6470943042755127\n",
            "Training Epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.77it/s]\n",
            "100%|| 157/157 [00:09<00:00, 15.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.820775 train_loss =  0.5187139167785645 Validation_acc =  0.782 Validation_loss =  0.6603468955993652\n",
            "Training Epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 17.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.8305 train_loss =  0.4833750304222107 Validation_acc =  0.7799 Validation_loss =  0.6645062547683716\n",
            "Training Epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 16.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.838225 train_loss =  0.4697833930015564 Validation_acc =  0.7831 Validation_loss =  0.6548446937561035\n",
            "Training Epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.847325 train_loss =  0.44156955900192263 Validation_acc =  0.7864 Validation_loss =  0.650598539352417\n",
            "Training Epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.77it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.848375 train_loss =  0.43029700763225553 Validation_acc =  0.7875 Validation_loss =  0.6460254604339599\n",
            "Training Epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.852975 train_loss =  0.4176544392585754 Validation_acc =  0.7923 Validation_loss =  0.6373544242858886\n",
            "Training Epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:09<00:00, 15.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.85905 train_loss =  0.4069040879607201 Validation_acc =  0.7929 Validation_loss =  0.6345718376159668\n",
            "Training Epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.79it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.864875 train_loss =  0.38402257335186 Validation_acc =  0.7909 Validation_loss =  0.6353577842712402\n",
            "Training Epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.77it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.871625 train_loss =  0.3692235598564148 Validation_acc =  0.8003 Validation_loss =  0.6275423719406128\n",
            "Training Epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:09<00:00, 16.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.871775 train_loss =  0.36247470680475236 Validation_acc =  0.7942 Validation_loss =  0.6276430068016052\n",
            "Training Epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 16.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.881 train_loss =  0.3383572028398514 Validation_acc =  0.8047 Validation_loss =  0.6130931441307068\n",
            "Training Epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.885075 train_loss =  0.3263112630486488 Validation_acc =  0.8019 Validation_loss =  0.6280312598228455\n",
            "Training Epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.80it/s]\n",
            "100%|| 157/157 [00:10<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.887525 train_loss =  0.3202081279635429 Validation_acc =  0.8062 Validation_loss =  0.6127872571945191\n",
            "Training Epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 625/625 [01:20<00:00,  7.78it/s]\n",
            "100%|| 157/157 [00:09<00:00, 15.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.890575 train_loss =  0.31414549440145495 Validation_acc =  0.7976 Validation_loss =  0.6583480808258056\n",
            "Test Accuracy is  0.8169\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation_acc</td><td></td></tr><tr><td>Validation_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation_acc</td><td>0.7976</td></tr><tr><td>Validation_loss</td><td>0.65835</td></tr><tr><td>train_acc</td><td>0.89058</td></tr><tr><td>train_loss</td><td>0.31415</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">light-blaze-20</strong> at: <a href='https://wandb.ai/dl-codes/dl_project_2023/runs/oscoffk5' target=\"_blank\">https://wandb.ai/dl-codes/dl_project_2023/runs/oscoffk5</a><br/>Synced 5 W&B file(s), 1 media file(s), 321 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230510_010304-oscoffk5/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}